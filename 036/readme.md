## Kubernetes in Actions

## question

1. å› ä¸º rc æ˜¯å’Œ label ç»‘å®šçš„ï¼Œé‚£ä¹ˆ kubernetes é›†ç¾¤ä¸­æ˜¯å¦ä¼šå­˜åœ¨ä¸¤ä¸ª label ä¸€æ¨¡ä¸€æ ·çš„ podï¼Ÿ
2. ä¸ºä»€ä¹ˆéœ€è¦ä½¿ç”¨ endpointï¼Ÿ
3. ClusterIPã€PodIdã€ExternalIP çš„åŒºåˆ«ï¼Ÿ

### ClusterIPã€PodIdã€ExternalIP çš„åŒºåˆ«ï¼Ÿ

[k8sä¹‹PodIPã€ClusterIPå’ŒExternalIP](https://www.cnblogs.com/embedded-linux/p/12657128.html)

## references

- [Kubernetes Components](https://kubernetes.io/docs/concepts/overview/components/)
- [Glossary - a comprehensive, standardized list of Kubernetes terminology](https://kubernetes.io/docs/reference/glossary/)
- [Kubernetesçš„ä¸‰ç§å¤–éƒ¨è®¿é—®æ–¹å¼ï¼šNodePortã€LoadBalancer å’Œ Ingress](http://dockone.io/article/4884)

## roadmap

![k8s-roadmap](k8s-roadmap.png)

## components-of-kubernetes

![components-of-kubernetes](components-of-kubernetes.svg)

## minikube

```bash
minikube start 
	--cpus=2
	--memory=2048mb
	--registry-mirror=https://t65rjofu.mirror.aliyuncs.com
	--driver=virtualbox
	--nodes=3
```

## 1. Kubernetes ä»‹ç»

1. Kubernetes å¯ä»¥è¢«å½“åšé›†ç¾¤çš„ä¸€ä¸ªæ“ä½œç³»ç»Ÿæ¥çœ‹å¾…ï¼›
2. å½“ `API server` å¤„ç†åº”ç”¨çš„æè¿°æ—¶ï¼Œ`scheduler` è°ƒåº¦æŒ‡å®šç»„çš„å®¹å™¨åˆ°å¯ç”¨çš„å·¥ä½œèŠ‚ç‚¹ä¸Šï¼Œè°ƒåº¦æ˜¯åŸºäºæ¯ç»„æ‰€éœ€çš„è®¡ç®—èµ„æºï¼Œä»¥åŠè°ƒåº¦æ—¶æ¯ä¸ªèŠ‚ç‚¹æœªåˆ†é…çš„èµ„æºã€‚ç„¶åï¼Œé‚£äº›èŠ‚ç‚¹ä¸Šçš„ `Kubelet` æŒ‡ç¤ºå®¹å™¨è¿è¡Œæ—¶ï¼ˆä¾‹å¦‚Docker ï¼‰æ‹‰å–æ‰€éœ€çš„é•œåƒå¹¶è¿è¡Œå®¹å™¨ã€‚

![k8s work](kubernetes ä½“ç³»ç»“æ„çš„åŸºæœ¬æ¦‚è¿°å’Œåœ¨å®ƒä¹‹ä¸Šè¿è¡Œçš„åº”ç”¨ç¨‹åº.png)

## 2. å¼€å§‹ä½¿ç”¨ kubernetes å’Œ docker

```bash
docker run busybox echo â€Hello worldâ€
```

![busybox](busybox.png)

#### 2.1.2 åˆ›å»ºä¸€ä¸ªç®€å•çš„Node.js åº”ç”¨

```javascript
const http = require('http');
const os   = require('os');

console.log('Kubia server starting...')

var handler = function(request, response) {
	console.log("Received request from " + request.connection.remoteAddress)
	response.writeHead(200)
	response.end("You've hit " + os.hostname() + "\n")
};

var www = http.createServer(handler);
www.listen(8080);
```

#### æ„å»ºå®¹å™¨é•œåƒ

> åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `node:7` ä½œä¸ºåŸºç¡€é•œåƒï¼Œå› ä¸ºå¯¹äº node åº”ç”¨æ¥è¯´ node åŒ…å«äº†è¿è¡Œåº”ç”¨æ‰€éœ€çš„ä¸€åˆ‡ï¼Œæ‰€ä»¥æˆ‘ä»¬æ— éœ€ä½¿ç”¨ï¼š
>
> app.js
>
> node
>
> linux
>
> è¿™ç§å±‚çº§çš„é•œåƒç»“æ„ã€‚

```dockerfile
FROM node:7

ADD app.js /app.js
ENTRYPOINT ["node", "app.js"]
```

```bash
# æ‰“åŒ…é•œåƒ
# åœ¨å½“å‰ç›®å½•æ„å»ºä¸€ä¸ªå»º kubia çš„é•œåƒï¼Œdocker ä¼šåœ¨å½“å‰ç›®å½•å¯»æ‰¾ Dockerfile ç„¶åæ„å»º docker é•œåƒã€‚
docker build -t kubia .

docker images kubia
# REPOSITORY   TAG       IMAGE ID       CREATED              SIZE
# kubia        latest    4369322ecec2   About a minute ago   660MB

# è¿è¡Œé•œåƒ
docker run --name kubia-container -p 8080:8080 -d kubia

# å¤åˆ¶ kubia é•œåƒï¼Œå¹¶ä½¿ç”¨ luksa/kubia ä½œä¸ºåå­— 
docker tag kubia luksa/kubia
```

#### é…ç½® kubernetes é›†ç¾¤

```bash
# å¯åŠ¨ minikube è™šæ‹Ÿæœº
minikube start
#ğŸ˜„  minikube v1.21.0 on Darwin 10.15.7
#âœ¨  Using the docker driver based on existing profile
#ğŸ‘  Starting control plane node minikube in cluster minikube
#ğŸšœ  Pulling base image ...
#ğŸƒ  Updating the running docker "minikube" container ...
#ğŸ³  Preparing Kubernetes v1.20.7 on Docker 20.10.7 ...
#ğŸ”  Verifying Kubernetes components...
#    â–ª Using image kubernetesui/metrics-scraper:v1.0.4
#    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
#    â–ª Using image kubernetesui/dashboard:v2.1.0
#ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass, dashboard
#ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

# å¯åŠ¨3ä¸ªè™šæ‹ŸèŠ‚ç‚¹
minikube start --nodes=3
# ä¸‹é¢çš„å‘½ä»¤ä¼šæŒ‡å®š profile ä¸º multinode-demo
#minikube start --nodes=3 -p multinode-demo

# éªŒè¯é›†ç¾¤æ˜¯å¦æ­£å¸¸å·¥ä½œ
kubectl cluster-info

# åˆ—å‡ºé›†ç¾¤èŠ‚ç‚¹
kubectl get nodes
# NAME                  STATUS     ROLES                  AGE     VERSION
# multinode-demo1       Ready      control-plane,master   2m24s   v1.20.7
# multinode-demo1-m02   Ready      <none>                 72s     v1.20.7
# multinode-demo1-m03   NotReady   <none>                 9s      v1.20.7

# æŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€
kubectl describe node multinode-demo1
```

#### åœ¨ kubernetes ä¸Šéƒ¨ç½²ç¬¬ä¸€ä¸ªåº”ç”¨

> ä¸€ä¸ª pod æ˜¯ä¸€ç»„ç´§å¯†ç›¸å…³çš„å®¹å™¨ï¼Œä»–ä»¬æ€»æ˜¯è¿è¡Œåœ¨åŒä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹ä¸Šï¼Œä»¥åŠåŒä¸€ä¸ª linux namespaceã€‚

```bash
# --image=luksa/kubia æ˜¾ç¤ºçš„æ˜¯æŒ‡å®šè¦è¿è¡Œçš„å®¹å™¨é•œåƒï¼Œ
# --port=8080 å‘Šè¯‰ kubernetes åº”ç”¨æ­£åœ¨ç›‘å¬ 8080
kubectl run kubia --image=luksa/kubia --port=8080

# åˆ—å‡º pod
kubectl get pods
# NAME    READY   STATUS    RESTARTS   AGE
# kubia   1/1     Running   0          5m41s

# Now kubectl run command creates standalone pod without ReplicationController. 
kubectl expose pod kubia --type=LoadBalancer --name kubia-http

# Connect to LoadBalancer services
minikube tunnel
# ğŸƒ  Starting tunnel for service kubia-http.

kubectl get services
# NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
# kubernetes   ClusterIP      10.96.0.1       <none>        443/TCP          150m
# kubia-http   LoadBalancer   10.101.223.38   <pending>     8080:32161/TCP   96s
```

![åœ¨kubernetesä¸­è¿è¡Œluksa/kubiaå®¹å™¨](åœ¨kubernetesä¸­è¿è¡Œluksa:kubiaå®¹å™¨.png)

## 3. pod : è¿è¡Œäº kubernetes ä¸­çš„å®¹å™¨

### 3.1 ä»‹ç» pod

> ä¸ºä½•éœ€è¦ pod è¿™ç§å®¹å™¨ï¼Ÿ
>
> ä¸ºä½•ä¸ç›´æ¥ä½¿ç”¨å®¹å™¨ï¼Ÿ
>
> ä¸ºä½•ç”šè‡³éœ€è¦åŒæ—¶è¿è¡Œå¤šä¸ªå®¹å™¨ï¼Ÿéš¾é“ä¸èƒ½æŠŠæ‰€æœ‰çš„è¿›ç¨‹éƒ½æ”¾åœ¨ä¸€ä¸ªå®¹å™¨ä¸­å—ï¼Ÿ

å®¹å™¨è¢«è®¾è®¡ä¸ºæ¯ä¸ªå®¹å™¨åªè¿è¡Œä¸€ä¸ªè¿›ç¨‹ï¼ˆé™¤éè¿›ç¨‹è‡ªå·±äº§ç”Ÿæ–°çš„è¿›ç¨‹ï¼‰ã€‚å¦‚æœåœ¨å•ä¸ªå®¹å™¨ä¸­è¿è¡Œå¤šä¸ªä¸ç›¸å…³çš„è¿›ç¨‹ï¼Œé‚£ä¹ˆä¿æŒæ‰€æœ‰çš„è¿›ç¨‹è¿è¡Œã€ç®¡ç†ä»–ä»¬çš„æ—¥å¿—å°†ä¼šæ˜¯æˆ‘ä»¬çš„è´£ä»»ã€‚å½“å®¹å™¨å´©æºƒæ—¶ï¼Œå®¹å™¨å†…åŒ…å«çš„è¿›ç¨‹å…¨éƒ¨è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡ºä¸­ï¼Œæ­¤æ—¶æˆ‘ä»¬å¾ˆéš¾ç¡®å®šæ¯ä¸ªè¿›ç¨‹åˆ†åˆ«è®°å½•äº†ä»€ä¹ˆã€‚

ç”±äºä¸èƒ½å°†å¤šä¸ªè¿›ç¨‹æ”¾åœ¨ä¸€ä¸ªå•ç‹¬çš„å®¹å™¨ä¸­ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å¦ä¸€ç§æ›´é«˜çº§çš„ç»“æ„æ¥å°†å®¹å™¨ç»‘å®šåˆ°ä¸€èµ·ï¼Œå¹¶å°†ä»–ä»¬ä½œä¸ºä¸€ä¸ªå•å…ƒè¿›ç¨‹ç®¡ç†ã€‚

**å®¹å™¨ä¹‹é—´æ˜¯å®Œå…¨éš”ç¦»çš„ï¼Œæˆ‘ä»¬çš„æœŸæœ›æ˜¯éš”ç¦»å®¹å™¨ç»„è€Œä¸æ˜¯å•ä¸ªå®¹å™¨ï¼Œå¹¶ä¸”è®©æ¯ä¸ªå®¹å™¨ç»„å†…çš„å®¹å™¨å…±äº«ä¸€äº›èµ„æºï¼Œè€Œä¸æ˜¯å…¨éƒ¨ã€‚kubernetes é€šè¿‡é…ç½® docker è®©ä¸€ä¸ª pod å†…çš„æ‰€æœ‰å®¹å™¨å…±äº«ç›¸åŒçš„linux namespaceï¼Œè€Œä¸æ˜¯æ¯ä¸ªå®¹å™¨éƒ½æœ‰è‡ªå·±çš„ä¸€ç»„ namespaceã€‚**

ä¸€ä¸ª pod ä¸­çš„å®¹å™¨è¿è¡Œäºç›¸åŒçš„ network namespaceï¼Œå› æ­¤ä»–ä»¬äº«æœ‰ç›¸åŒçš„ipå’Œ portã€‚å› æ­¤å¯¹äºåœ¨åŒä¸€ä¸ª pod ä¸‹çš„å¤šä¸ªè¿›ç¨‹ä¸èƒ½ç»‘å®šåˆ°ç›¸åŒçš„ç«¯å£ã€‚

![podé—´çš„ç½‘ç»œæ¨¡å‹](podé—´çš„ç½‘ç»œæ¨¡å‹.png)

åŒä¸€ä¸ª kubernetes é›†ç¾¤çš„ pod åœ¨åŒä¸€ä¸ª **å…±äº«ç½‘ç»œåœ°å€ç©ºé—´**ï¼Œè¿™æ„å‘³ç€æ¯ä¸ª pod éƒ½å¯ä»¥é€šè¿‡å…¶ä»– pod çš„ ip åœ°å€æ¥å®ç°äº’ç›¸è®¿é—®ã€‚

##### å°†å¤šå±‚åº”ç”¨åˆ†æ•£åˆ°å¤šä¸ªpodä¸­

> å¯¹äºä¸€ä¸ªæœ‰å‰ç«¯æœåŠ¡å’Œåç«¯æ•°æ®åº“ç»„æˆçš„å¤šå±‚åº”ç”¨ç¨‹åºï¼Œåº”è¯¥é…ç½®æˆå•ä¸ªpodè¿˜æ˜¯å¤šä¸ªpodå‘¢ï¼Ÿ

å¦‚æœæˆ‘ä»¬æ”¾åˆ°ä¸€ä¸ª pod ä¸­ï¼Œé‚£ä¹ˆæ„å‘³ç€å‰ç«¯å’Œåç«¯çš„æœåŠ¡æ°¸è¿œåªèƒ½åœ¨ä¸€å°æœºå™¨ä¸Šæ‰§è¡Œï¼Œæ— æ³•å……åˆ†çš„æé«˜åŸºç¡€æ¶æ„çš„ä½¿ç”¨ç‡ã€‚

å¦å¤–ï¼Œkubernetes çš„æ‰©ç¼©å®¹ä¹Ÿæ˜¯åŸºäº pod çš„ï¼Œå‰ç«¯å’Œåç«¯çš„æœåŠ¡æ”¾åœ¨ä¸€ä¸ª pod ä¸‹æ„å‘³ç€æˆ‘ä»¬æ— æ³•é’ˆå¯¹å‰ç«¯æœåŠ¡ä»¥åŠåç«¯æœåŠ¡çš„éœ€æ±‚è¿›è¡Œæ‰©ç¼©å®¹ã€‚

##### ä½•æ—¶åœ¨ pod ä¸­ä½¿ç”¨å¤šä¸ªå®¹å™¨

> ä½¿ç”¨å•ä¸ª pod åŒ…å«å¤šä¸ªå®¹å™¨çš„ä¸»è¦åŸå› æ˜¯ï¼šåº”ç”¨å¯èƒ½ç”±ä¸€ä¸ªä¸»è¿›ç¨‹å’Œä¸€ä¸ªæˆ–è€…å¤šä¸ªè¾…åŠ©è¿›ç¨‹ç»„æˆã€‚

ä¾‹å¦‚åœ¨å¾®æœåŠ¡æ¶æ„ä¸­çš„ï¼Œä¸»è¿›ç¨‹æ˜¯ä¸šåŠ¡é€»è¾‘ï¼Œè¾…åŠ©è¿›ç¨‹æ˜¯ envoyï¼ˆæˆ–è€…å…¶ä»–ç½‘å…³ï¼‰ã€‚

![å‰ç«¯æœåŠ¡å’Œåç«¯æœåŠ¡æ¶æ„](å‰ç«¯æœåŠ¡å’Œåç«¯æœåŠ¡æ¶æ„.png)

### 3.2 ä»¥ yaml æˆ– json æè¿°æ–‡ä»¶åˆ›å»º pod

[kubernetes docs](https://kubernetes.io/docs/reference/)

```bash
# ä½¿ç”¨ -o yaml é€‰é¡¹è·å– pod çš„æ•´ä¸ªå®šä¹‰ yaml
kubectl get po kubia -o yaml
```

```yaml
# yaml æè¿°æ–‡ä»¶æ‰€ä½¿ç”¨çš„ kubernetes API ç‰ˆæœ¬
apiVersion: v1
# kubernetes å¯¹è±¡èµ„æºç±»å‹
kind: Pod
# pod å…ƒæ•°æ®ï¼ˆåç§°ã€æ ‡ç­¾å’Œæ³¨è§£ç­‰ï¼‰
metadata:
  creationTimestamp: "2021-10-27T07:33:55Z"
  labels:
    run: kubia
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:labels:
          .: {}
          f:run: {}
      f:spec:
        f:containers:
          k:{"name":"kubia"}:
            .: {}
            f:image: {}
            f:imagePullPolicy: {}
            f:name: {}
            f:ports:
              .: {}
              k:{"containerPort":8080,"protocol":"TCP"}:
                .: {}
                f:containerPort: {}
                f:protocol: {}
            f:resources: {}
            f:terminationMessagePath: {}
            f:terminationMessagePolicy: {}
        f:dnsPolicy: {}
        f:enableServiceLinks: {}
        f:restartPolicy: {}
        f:schedulerName: {}
        f:securityContext: {}
        f:terminationGracePeriodSeconds: {}
    manager: kubectl-run
    operation: Update
    time: "2021-10-27T07:33:55Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:conditions:
          k:{"type":"ContainersReady"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Initialized"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
          k:{"type":"Ready"}:
            .: {}
            f:lastProbeTime: {}
            f:lastTransitionTime: {}
            f:status: {}
            f:type: {}
        f:containerStatuses: {}
        f:hostIP: {}
        f:phase: {}
        f:podIP: {}
        f:podIPs:
          .: {}
          k:{"ip":"10.244.2.2"}:
            .: {}
            f:ip: {}
        f:startTime: {}
    manager: kubelet
    operation: Update
    time: "2021-10-27T07:35:16Z"
  name: kubia
  namespace: default
  resourceVersion: "1110"
  uid: 29ed7476-3fff-4ddb-93c8-90a1fa81fda9
# pod è§„æ ¼/å†…å®¹ï¼ˆpod çš„å®¹å™¨åˆ—è¡¨ã€volumn ç­‰ï¼‰
spec:
  containers:
  - image: luksa/kubia
    imagePullPolicy: Always
    name: kubia
    ports:
    - containerPort: 8080
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-zmmm2
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: minikube-m03
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: default-token-zmmm2
    secret:
      defaultMode: 420
      secretName: default-token-zmmm2
# pod æœºå™¨å†…éƒ¨å®¹å™¨çš„è¯¦ç»†çŠ¶æ€
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2021-10-27T07:33:55Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2021-10-27T07:35:16Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2021-10-27T07:35:16Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2021-10-27T07:33:55Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://56a21a88860f2a9afe9a4eaf66ae69f20eac03d238ae8d11738c052cdf5e1ae6
    image: luksa/kubia:latest
    imageID: docker-pullable://luksa/kubia@sha256:3f28e304dc0f63dc30f273a4202096f0fa0d08510bd2ee7e1032ce600616de24
    lastState: {}
    name: kubia
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2021-10-27T07:35:16Z"
  hostIP: 192.168.49.4
  phase: Running
  podIP: 10.244.2.2
  podIPs:
  - ip: 10.244.2.2
  qosClass: BestEffort
  startTime: "2021-10-27T07:33:55Z"
```

##### kubia-manual.yaml

> ä¸‹é¢çš„ yaml æè¿°äº†å¦‚ä¸‹çš„ä¸€ä¸ª podï¼š
>
> 1. ä½¿ç”¨ kubernetes v1 çš„ api
> 2. èµ„æºç±»å‹æ˜¯ä¸€ä¸ª pod
> 3. å®¹å™¨çš„åç§°æ˜¯ kubia-manual
> 4. pod åŸºäºåä¸º luksa/kubia çš„é•œåƒç»„æˆï¼Œç›‘å¬ç«¯å£ 8080 

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-manual
spec:
  containers:
    - image: luksa/kubia
      name: kubia
      ports:
        - containerPort: 8080
          protocol: TCP
```

```bash
# è§£é‡Š pod çš„å­—æ®µ
k explain pods
#KIND:     Pod
#VERSION:  v1
#
#DESCRIPTION:
#     Pod is a collection of containers that can run on a host. This resource is
#     created by clients and scheduled onto hosts.
#
#FIELDS:
#   apiVersion     <string>
#     APIVersion defines the versioned schema of this representation of an
#     object. Servers should convert recognized schemas to the latest internal
#     value, and may reject unrecognized values. More info:
#     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources
#
#   kind     <string>
#     Kind is a string value representing the REST resource this object
#     represents. Servers may infer this from the endpoint the client submits
#     requests to. Cannot be updated. In CamelCase. More info:
#     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds
#
#   metadata <Object>
#     Standard object's metadata. More info:
#     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata
#
#   spec     <Object>
#     Specification of the desired behavior of the pod. More info:
#     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status
#
#   status   <Object>
#     Most recently observed status of the pod. This data may not be up to date.
#     Populated by the system. Read-only. More info:
#     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status

# è§£é‡Š pods ä¸‹çš„ spec
k explain pods.spec
```

#### 3.2.3 ä½¿ç”¨ kubectl create æ¥åˆ›å»º pod

```bash
# -f è¡¨ç¤ºèµ„æºæ–‡ä»¶
k create -f kubia-manual.yaml

k get pods
#NAME           READY   STATUS    RESTARTS   AGE
#kubia          1/1     Running   0          79m
#kubia-manual   1/1     Running   0          37s
```

> æˆ‘ä»¬é€šè¿‡ `kubia-manual.yaml` åˆ›å»ºçš„ pod **kubia-manual**ï¼Œå’Œæˆ‘ä»¬æœ€å¼€å§‹æ‰‹åŠ¨åˆ›å»ºçš„ **kubia**ï¼Œä¸¤ä¸ªéƒ½ç»‘å®šåˆ°äº† 8080 ç«¯å£ã€‚ä½†æ˜¯æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œä¸åŒçš„ pod ä¹‹é—´æ˜¯åœ¨ä¸åŒçš„ network namespace ä¸‹ï¼Œæ‰€ä»¥ä»–ä»¬ä¸ä¼šå†²çªã€‚

```bash
# æŸ¥çœ‹ pod æ—¥å¿—
kubectl logs kubia-manual

# å¦‚æœ pod ä¸­æœ‰å…¶ä»–å®¹å™¨ï¼Œå¯ä»¥é€šè¿‡ -c æŒ‡å®šå®¹å™¨
kubectl logs kubia-manual -c kubia
```

#### 3.2.5 å‘ pod å‘é€è¯·æ±‚

å‰é¢æˆ‘ä»¬ä½¿ç”¨äº† `kubectl expose` å‘½ä»¤åˆ›å»ºäº†ä¸€ä¸ª serviceï¼Œä»¥ä¾¿äºåœ¨å¤–éƒ¨è®¿é—® podã€‚

é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ **ç«¯å£è½¬å‘** æ¥å®ç°è¿™ä¸ªåŠŸèƒ½ã€‚

```bash
# å°†æœ¬æœºå™¨çš„ 8888 ç«¯å£è½¬å‘åˆ° kubia-manual çš„ 8080 ç«¯å£
kubectl port-forward kubia-manual 8888:8080
```

![k8s port-forward](k8s port-forward.png)

### 3.3 ä½¿ç”¨æ ‡ç­¾ç»„ç»‡ pod

> åœ¨å®é™…çš„åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªç®€å•çš„æ–¹æ³•æ¥åŒºåˆ†æ‰€æœ‰çš„ podã€‚
>
> ä¾‹å¦‚ï¼Œåœ¨ç°åº¦å‘å¸ƒä¸­ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“å“ªäº› pod æ˜¯å·²ç»ç°åº¦çš„ï¼Œå“ªäº›æ˜¯æ²¡æœ‰ç°åº¦çš„ã€‚

#### 3.3.1 ä»‹ç»æ ‡ç­¾

> 1. pod å¯ä»¥ç»„ç»‡ kubernetes çš„æ‰€æœ‰èµ„æºï¼›
> 2. pod æ˜¯å¯ä»¥é™„åŠ åˆ°èµ„æºçš„ä»»æ„é”®å€¼å¯¹ï¼›

å‡è®¾æˆ‘ä»¬ç°åœ¨çš„ pod æœ‰ä¸¤ä¸ªæ ‡ç­¾ï¼š

1. app
2. relï¼šæ˜¾ç¤ºåº”ç”¨ç¨‹åºç‰ˆæœ¬æ˜¯ stableã€beta è¿˜æ˜¯ canaryã€‚

![ä½¿ç”¨æ ‡ç­¾ç»„ç»‡pod](ä½¿ç”¨æ ‡ç­¾ç»„ç»‡pod.png)

#### 3.3.2 åˆ›å»ºpodæ—¶æŒ‡å®šæ ‡ç­¾

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-manual-with-labels
  # æŒ‡å®šæ ‡ç­¾
  labels:
    creation_method: manual
    env: prod
spec:
  containers:
    - image: luksa/kubia
      name: kubia
      ports:
        - containerPort: 8080
          protocol: TCP
```

```bash
# åˆ›å»ºå¸¦ labels çš„ pod
k create -f kubia-manual-with-labels.yaml

k get po --show-labels
#NAME                       READY   STATUS    RESTARTS   AGE     LABELS
#kubia                      1/1     Running   0          135m    run=kubia
#kubia-manual               1/1     Running   0          57m     <none>
#kubia-manual-with-labels   1/1     Running   0          2m11s   creation_method=manual,env=prod

# -L å°†æˆ‘ä»¬æ„Ÿå…´è¶£çš„æ ‡ç­¾æ˜¾ç¤ºåœ¨å¯¹åº”çš„åˆ—ä¸­
k get po -L creation_method,env
#NAME                       READY   STATUS    RESTARTS   AGE     CREATION_METHOD   ENV
#kubia                      1/1     Running   0          137m
#kubia-manual               1/1     Running   0          58m
#kubia-manual-with-labels   1/1     Running   0          3m43s   manual            prod

# ä½¿ç”¨ selector è¿‡æ»¤
kubectl get pods -l creation_method=manual
#NAME                       READY   STATUS    RESTARTS   AGE
#kubia-manual-with-labels   1/1     Running   0          6m23s
```

#### 3.3.3 ä¿®æ”¹ç°æœ‰ pod æ ‡ç­¾

```bash
# ä¿®æ”¹æ ‡ç­¾
k label po kubia-manual creation_method=manual

kubectl get pods -l creation_method=manual
#NAME                       READY   STATUS    RESTARTS   AGE
#kubia-manual               1/1     Running   0          65m
#kubia-manual-with-labels   1/1     Running   0          10m

# ä¿®æ”¹æ ‡ç­¾æ—¶å¿…é¡»å¢åŠ  --overwrite
k label po kubia-manual-with-labels env=debug --overwrite

k get pods -l env=debug -L creation_method,env
#NAME                       READY   STATUS    RESTARTS   AGE     CREATION_METHOD   ENV
#kubia-manual-with-labels   1/1     Running   0          2m36s   manual            debug
```

#### 3.4 ä½¿ç”¨æ ‡ç­¾é€‰æ‹©å™¨

```bash
# é€‰æ‹©æ ‡ç­¾
k get po -l creation_method=manual

# é€‰æ‹©æ ‡ç­¾ env=deubg
k get po -l env=debug
#NAME                       READY   STATUS    RESTARTS   AGE
#kubia-manual-with-labels   1/1     Running   0          4m47s

# é€‰æ‹©ä¸åŒ…å« env æ ‡ç­¾çš„ pod
k get po -l '!env' --show-labels
#NAME           READY   STATUS    RESTARTS   AGE     LABELS
#kubia          1/1     Running   0          5m56s   run=kubia
#kubia-manual   1/1     Running   0          5m45s   <none>
```

- env!=debug
- env in (prd, dev)
- env notin(prd, dev)
- Creation_method=manual,env=debug

#### 3.5 ä½¿ç”¨æ ‡ç­¾å’Œé€‰æ‹©å™¨æ¥çº¦æŸ pod è°ƒåº¦

> å½“æˆ‘ä»¬å¸Œæœ›æ§åˆ¶ pod çš„è°ƒåº¦çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¸ä¼šè¯´æ˜ pod åº”è¯¥è¢«è°ƒåº¦åˆ°å“ªä¸ªèŠ‚ç‚¹ä¸Šï¼Œè¿™ä¼šä½¿å¾—æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºå’ŒåŸºç¡€æ¶æ„å¼ºè€¦åˆã€‚
>
> æˆ‘ä»¬åº”è¯¥ **æè¿°åº”ç”¨å¯¹èŠ‚ç‚¹çš„éœ€æ±‚ï¼Œä½¿å¾— kubernetes é€‰æ‹©ä¸€äº›ç¬¦åˆè¿™äº›éœ€æ±‚çš„èŠ‚ç‚¹ã€‚**
>
> æ ‡ç­¾å¯ä»¥é™„åŠ åˆ° kubernetes çš„ä»»æ„å¯¹è±¡ä¸Šï¼Œ**è¿™ä¹ŸåŒ…æ‹¬äº†æˆ‘ä»¬æ–°å¢åŠ çš„èŠ‚ç‚¹ã€‚**

#### 3.5.1 ä½¿ç”¨æ ‡ç­¾åˆ†ç±»å·¥ä½œèŠ‚ç‚¹

```bash
# æŸ¥è¯¢æ‰€æœ‰èŠ‚ç‚¹
k get nodes --show-labels
#NAME           STATUS   ROLES                  AGE   VERSION   LABELS
#minikube       Ready    control-plane,master   19h   v1.20.7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,gpu=true,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,minikube.k8s.io/commit=76d74191d82c47883dc7e1319ef7cebd3e00ee11,minikube.k8s.io/name=minikube,minikube.k8s.io/updated_at=2021_10_27T15_26_02_0700,minikube.k8s.io/version=v1.21.0,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=
#minikube-m02   Ready    <none>                 25m   v1.20.7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube-m02,kubernetes.io/os=linux
#minikube-m03   Ready    <none>                 25m   v1.20.7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube-m03,kubernetes.io/os=linux

# ä¸º name=minikube çš„èŠ‚ç‚¹å¢åŠ æ ‡ç­¾ gpu=true
k label nodes minikube gpu=true

k get nodes -l gpu=true
#NAME       STATUS   ROLES                  AGE   VERSION
#minikube   Ready    control-plane,master   19h   v1.20.7
```

#### 3.5.2 å°† pod è°ƒåº¦åˆ°ç‰¹å®šèŠ‚ç‚¹

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  nodeSelector:
    gpu: "true"
  containers:
    - image: luksa/kubia
      name: kubia
```

#### 3.5.3 è°ƒåº¦åˆ°ä¸€ä¸ªç‰¹å®šèŠ‚ç‚¹

> æ¯ä¸ª node åŒ…å«ä¸€ä¸ªå”¯ä¸€æ ‡ç­¾ï¼š`kubernetes.io/hostname`

```bash
k get nodes -L kubernetes.io/hostname
#NAME           STATUS   ROLES                  AGE   VERSION   HOSTNAME
#minikube       Ready    control-plane,master   19h   v1.20.7   minikube
#minikube-m02   Ready    <none>                 54m   v1.20.7   minikube-m02
#minikube-m03   Ready    <none>                 53m   v1.20.7   minikube-m03
```

### 3.6 æ³¨è§£ pod

1. æ³¨è§£ä¹Ÿæ˜¯é”®å€¼å¯¹ï¼›
2. æ³¨è§£ä¸èƒ½åƒæ ‡ç­¾ä¸€æ ·å¯¹å¯¹è±¡è¿›è¡Œåˆ†ç»„ï¼›
3. ä¸€èˆ¬æ¥è¯´ï¼Œæ–°åŠŸèƒ½çš„ alpha å’Œ beta ç‰ˆæœ¬ä¸ä¼šå‘APIå¯¹è±¡å¼•å…¥ä»»ä½•æ–°çš„å­—æ®µï¼Œå› æ­¤ä½¿ç”¨çš„æ˜¯æ³¨è§£è€Œä¸æ˜¯å­—æ®µï¼Œä¸€æ—¦ç¡®å®šä¼šå¼•å…¥æ–°çš„å­—æ®µå¹¶åºŸå¼ƒæ³¨è§£ï¼›

#### 3.6.1 æŸ¥æ‰¾å¯¹è±¡çš„æ³¨è§£

```bash
# ä¸º kubia-manual æ·»åŠ æ³¨è§£
k annotate pod kubia-manual mycompany.com/someannotation="foo bar"

k get pods kubia-manual -o yaml | head -n 10
#apiVersion: v1
#kind: Pod
#metadata:
#  annotations:
#    mycompany.com/someannotation: foo bar
#  creationTimestamp: "2021-10-28T02:26:55Z"
#  managedFields:
#  - apiVersion: v1
#    fieldsType: FieldsV1
#    fieldsV1:
```

### 3.7 ä½¿ç”¨å‘½åç©ºé—´å¯¹èµ„æºè¿›è¡Œåˆ†ç»„

#### 3.7.1 äº†è§£å¯¹ namespace çš„éœ€æ±‚

é€šè¿‡ namesapceï¼Œæˆ‘ä»¬å¯ä»¥å°†åŒ…å«å¤§é‡ç»„ä»¶çš„å¤æ‚ç³»ç»Ÿæ‹†åˆ†ä¸ºæ›´å°çš„ä¸åŒç»„ï¼Œä¾‹å¦‚æˆ‘ä»¬å¯ä»¥å°†èµ„æºåˆ†é…ä¸º devï¼Œprd ä»¥åŠ QAã€‚

#### 3.7.2 å‘ç°å…¶ä»– namespace ä»¥åŠ pod

```bash
# æŸ¥è¯¢æ‰€æœ‰namespace
k get ns
#NAME                   STATUS   AGE
#default                Active   22h
#kube-node-lease        Active   22h
#kube-public            Active   22h
#kube-system            Active   22h
#kubernetes-dashboard   Active   22h

# æŸ¥è¯¢å¯¹åº” namespace ä¸‹çš„ pod
k get pods --namespace kube-system
#NAME                               READY   STATUS    RESTARTS   AGE
#coredns-74ff55c5b-klnsq            1/1     Running   1          22h
#etcd-minikube                      1/1     Running   1          22h
#kindnet-dpbdl                      1/1     Running   1          22h
#kindnet-f5sxx                      1/1     Running   1          22h
#kindnet-qn6vb                      1/1     Running   1          22h
#kube-apiserver-minikube            1/1     Running   1          22h
#kube-controller-manager-minikube   1/1     Running   1          22h
#kube-proxy-gvnp2                   1/1     Running   1          22h
#kube-proxy-mfpn4                   1/1     Running   1          22h
#kube-proxy-zqg4v                   1/1     Running   1          22h
#kube-scheduler-minikube            1/1     Running   1          22h
#storage-provisioner                1/1     Running   1          22h
```

#### 3.7.3 åˆ›å»ºä¸€ä¸ª namspace

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: custom-namespace
```

```bash
k create  -f custom-namespace.yaml
#namespace/custom-namespace created

# ä¹Ÿå¯ä»¥é€šè¿‡å‘½ä»¤è¡Œç›´æ¥åˆ›å»º
k create namespace custom-namespace-command
```

```bash
# åœ¨å¯¹åº”çš„ namespace ä¸‹åˆ›å»º pod
k create -f kubia-manual.yaml --namespace custom-namespace
```

#### 3.7.5 namespace æä¾›çš„éš”ç¦»

namespace å°†å¯¹è±¡åˆ†éš”åˆ°ä¸åŒçš„ç»„ï¼Œåªå…è®¸æˆ‘ä»¬å¯¹å±äºç‰¹å®š namespace çš„å¯¹è±¡è¿›è¡Œæ“ä½œï¼Œå•å®é™…ä¸Šå‘½åç©ºé—´ä¸æä¾›å¯¹æ­£åœ¨è¿è¡Œçš„å¯¹è±¡çš„ä»»ä½•éš”ç¦»ã€‚

ä¾‹å¦‚ï¼Œä¸¤ä¸ªä¸åŒçš„å‘½åç©ºé—´çš„ pod å®é™…ä¸Šæ˜¯å¯ä»¥äº’ç›¸é€šä¿¡çš„ã€‚

```bash
# è·å– custom-namespace ä¸‹çš„ pod
k get pods --namespace custom-namespace -o wide
#NAME           READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
#kubia-manual   1/1     Running   0          11m   10.244.1.4   minikube-m02   <none>           <none>

# è¿›å…¥ default ä¸‹çš„ pod
k exec -it kubia -- /bin/bash

curl http://10.244.1.4:8080
# You've hit kubia-manual
```

### 3.8 åœæ­¢å’Œç§»é™¤ pod

> åœ¨åˆ é™¤ pod çš„è¿‡ç¨‹ä¸­ï¼Œkubernetes å‘è¿›ç¨‹å‘é€ä¸€ä¸ª SIGTERM ä¿¡å·å¹¶ç­‰å¾…ä¸€å®šæ—¶é—´ä½¿å…¶æ­£å¸¸å…³é—­ã€‚
>
> å¦‚æœæ²¡æœ‰åŠæ—¶å…³é—­ï¼Œåˆ™é€šè¿‡ SIGKILL ç»ˆæ­¢è¯¥è¿›ç¨‹ã€‚

```bash
# åœæ­¢å’Œç§»é™¤ pod
k delete pods kubia-gpu
```

#### 3.8.2 ä½¿ç”¨æ ‡ç­¾é€‰æ‹©å™¨åˆ é™¤ pod

```bash
k delete po -l creation_method=manual
# pod "kubia-manual-with-labels" deleted

k delete ns custom-namespace
```

```bash
# åˆ é™¤æ“ä½œä¹Ÿåªä¼šåœ¨å½“å‰ namesapce æ‰§è¡Œ
k delete po kubia-manual

k get pods --namespace custom-namespace -o wide
#NAME           READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
#kubia-manual   1/1     Running   0          18m   10.244.1.4   minikube-m02   <none>           <none>

k delete po kubia-manual --namespace custom-namespace

k get pods --namespace custom-namespace -o wide
# No resources found in custom-namespace namespace.
```

#### 3.8.4 åˆ é™¤æ‰€æœ‰ pod

```bash
k delete po --all
```

## 4. å‰¯æœ¬æœºåˆ¶å’Œå…¶ä»–æ§åˆ¶å™¨ï¼šéƒ¨ç½²æ‰˜ç®¡çš„ pod

> åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸ä¼šæ‰‹åŠ¨åˆ›å»º podï¼Œè€Œæ˜¯åˆ›å»º ReplicationController æˆ– Deployment è¿™æ ·çš„èµ„æºï¼Œæ¥ç€ç”±ä»–ä»¬åˆ›å»ºå¹¶ç®¡ç†å®é™…çš„ podã€‚
>
> å› ä¸ºæ‰‹åŠ¨åˆ›å»ºçš„ podï¼Œè€Œä¸æ˜¯æ‰˜ç®¡çš„ pod å¯èƒ½ä¼šå­˜åœ¨å®¹ç¾æ–¹é¢çš„é—®é¢˜ã€‚

### 4.1 ä¿æŒ pod å¥åº·

> åªè¦å°† pod è°ƒåº¦åˆ°æŸä¸ªèŠ‚ç‚¹ï¼Œè¯¥èŠ‚ç‚¹ä¸Šçš„ `kubelet` å°±ä¼šè¿è¡Œ pod çš„å®¹å™¨ï¼Œä»æ­¤åªè¦è¯¥ pod å­˜åœ¨ï¼Œå°±ä¼šä¿æŒè¿è¡Œã€‚
>
> **å¦‚æœå®¹å™¨çš„ä¸»è¿›ç¨‹å´©æºƒï¼ˆOOM æˆ–è€…å› ä¸º BUG å¯¼è‡´çš„é‡å¯ç­‰ï¼‰ï¼Œkubelet ä¼šè‡ªåŠ¨é‡å¯åº”ç”¨ç¨‹åºã€‚**
>
> ä½†æ˜¯ï¼Œæˆ‘ä»¬è¿˜æ˜¯éœ€è¦é€šè¿‡æŸç§æ‰‹æ®µæ¥ä¿è¯ kubelet èƒ½å¤Ÿæ¢æµ‹å®¹å™¨çš„å­˜æ´»çŠ¶æ€ã€‚å› ä¸ºå‡è®¾æˆ‘ä»¬çš„åº”ç”¨å› ä¸ºæ— é™å¾ªç¯æˆ–è€…æ­»é”è€Œåœæ­¢å“åº”ï¼Œä¸ºäº†ç¡®ä¿åº”ç”¨ç¨‹åºåœ¨è¿™ç§æƒ…å†µä¸‹å¯ä»¥é‡æ–°å¯åŠ¨ï¼Œå¿…é¡»ä»å¤–éƒ¨ç¨‹åºæ£€æŸ¥åº”ç”¨ç¨‹åºçš„è¿è¡Œæƒ…å†µã€‚

#### 4.1.1 ä»‹ç»å­˜æ´»æ¢é’ˆï¼ˆliveness probeï¼‰

- HTTP GET æ¢é’ˆ
- TCP Socket æ¢é’ˆ
- Exec æ¢é’ˆ

#### 4.1.2 åˆ›å»ºåŸºäº HTTP çš„å­˜æ´»æ¢é’ˆ

```javascript
// æ¯äº”æ¬¡è¯·æ±‚ä¼šè¿”å›ä¸€æ¬¡ 500 é”™è¯¯ç 
const http = require('http');
const os = require('os');

console.log("Kubia server starting...");

var requestCount = 0;

var handler = function(request, response) {
  console.log("Received request from " + request.connection.remoteAddress);
  requestCount++;
  if (requestCount > 5) {
    response.writeHead(500);
    response.end("I'm not well. Please restart me!");
    return;
  }
  response.writeHead(200);
  response.end("You've hit " + os.hostname() + "\n");
};

var www = http.createServer(handler);
www.listen(8080);
```

```dockerfile
FROM node:7
ADD app.js /app.js
ENTRYPOINT ["node", "app.js"]
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-liveness
spec:
  containers:
    - image: luksa/kubia-unhealthy
      name: kubia
      livenessProbe:
        httpGet:
          path: /
          port: 8080
```

```bash
# æ‰“åŒ…é•œåƒ
docker build -t kubia-liveness .

docker tag kubia-liveness luksa/kubia-liveness

# åˆ›å»º pod
k create -f kubia-liveness-probe.yaml

# éš”ä¸€æ®µæ—¶é—´æŸ¥çœ‹ä¸€ä¸‹ pod çŠ¶æ€ï¼Œçœ‹çœ‹ pod æ˜¯å¦æœ‰é‡å¯
k get po kubia-liveness
#NAME             READY   STATUS    RESTARTS   AGE
#kubia-liveness   1/1     Running   1          3m34s

# æŸ¥çœ‹ pod çŠ¶æ€
k describe po kubia-liveness
```

![kubectl-desc.png](kubectl-desc.png)

#### 4.1.4 é…ç½®å­˜æ´»æ¢é’ˆçš„é™„åŠ å±æ€§

- delay
- timeout
- period
- failure

#### 4.1.5 åˆ›å»ºæœ‰æ•ˆçš„å­˜æ´»æ¢é’ˆ

> 1. å­˜æ´»æ¢é’ˆä¸åº”è¯¥ä¾èµ–äºä»»ä½•å¤–éƒ¨ç¨‹åºï¼šä¾‹å¦‚ï¼Œå½“æœåŠ¡å™¨æ— æ³•è¿æ¥åˆ°åç«¯æ•°æ®åº“æ—¶ï¼Œå‰ç«¯webæœåŠ¡å™¨ä¸åº”è¯¥è¿”å›å¤±è´¥ã€‚åç«¯æ•°æ®åº“çš„å­˜æ´»åº”è¯¥ç”±æ•°æ®åº“çš„å­˜æ´»æ¢é’ˆæ¥æ¢æµ‹ï¼›
> 2. å­˜æ´»æ¢é’ˆåº”è¯¥è¶³å¤Ÿè½»é‡ï¼Œé¿å…æ¶ˆè€—è¿‡å¤šçš„èµ„æºï¼›
> 3. å¦‚æœæ˜¯ä»»ä½•åŸºäº JVM æˆ–è€…ç±»ä¼¼çš„åº”ç”¨ï¼Œåº”è¯¥ä½¿ç”¨ HTTP GET å­˜è´§æ¢é’ˆï¼Œå¦‚æœæ˜¯ exec æ¢é’ˆä¼šå› ä¸ºå¯åŠ¨è¿‡ç¨‹è€Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºã€‚

### 4.2 äº†è§£ ReplicationController

> ReplicationController ç”¨äºç¡®ä¿ pod å§‹ç»ˆè¿è¡Œï¼Œå¹¶ä¸”ä¿è¯ pod çš„æ•°é‡ä¸å¤šä¸å°‘ã€‚

![ReplicationController é‡å»º pod](ReplicationController é‡å»º pod.png)

![ReplicationController çš„åè°ƒæµç¨‹](ReplicationController çš„åè°ƒæµç¨‹.png)

#### ReplicationController çš„ä¸‰ä¸ªéƒ¨åˆ†

- label selector
- replica count
- pod template

![ReplicationControllerçš„ä¸‰ä¸ªå…³é”®éƒ¨åˆ†](ReplicationControllerçš„ä¸‰ä¸ªå…³é”®éƒ¨åˆ†.png)

#### æ›´æ”¹æ§åˆ¶å™¨çš„æ ‡ç­¾é€‰æ‹©å™¨æˆ– pod æ¨¡æ¿çš„æ•ˆæœ

> æ›´æ”¹æ ‡ç­¾é€‰æ‹©å™¨å’Œ pod æ¨¡æ¿å¯¹ç°æœ‰ pod æ²¡æœ‰å½±å“ã€‚æ›´æ”¹æ ‡ç­¾é€‰æ‹©å™¨ä¼šä½¿å¾—ç°æœ‰çš„ pod è„±ç¦» ReplicationController çš„èŒƒå›´ï¼Œå› æ­¤ReplicationControllerä¼šåœæ­¢å…³æ³¨ä»–ä»¬ã€‚
>
> åœ¨åˆ›å»º pod ä¹‹åï¼ŒReplicationController ä¹Ÿä¸å…³å¿ƒ pod çš„å®é™…å†…å®¹ï¼ˆå®¹å™¨ç¯å¢ƒã€ç¯å¢ƒå˜é‡ç­‰ï¼‰ã€‚

> æ³¨æ„ï¼ŒReplicationController ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„ pod å®ä¾‹ï¼Œä¸æ­£åœ¨æ›¿æ¢çš„å®ä¾‹æ— å…³ã€‚

#### 4.2.2 åˆ›å»ºä¸€ä¸ª ReplicationController

> ä¸‹é¢çš„é…ç½®æ–‡ä»¶ä¸Šä¼ åˆ° API server æ—¶ï¼Œkubernetes ä¼šåˆ›å»ºä¸€ä¸ªåä¸º kubia çš„ rcï¼Œå®ƒç¡®ä¿ç¬¦åˆæ ‡ç­¾ app=kubia çš„ pod å§‹ç»ˆæ˜¯3ä¸ªã€‚

```yaml
apiVersion: v1
# è¿™é‡Œå®šä¹‰äº† rc
kind: ReplicationController
metadata:
  # rc çš„åå­—
  name: kubia
spec:
  # pod å®ä¾‹æ•°é‡
  replicas: 3
  # selector å†³å®šäº† rc çš„æ“ä½œå¯¹è±¡
  selector:
    app: kubia
  # åˆ›å»ºæ–° pod ä½¿ç”¨çš„æ¨¡æ¿
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
        - name: kubia
          image: luksa/kubia
          ports:
            - containerPort: 8080
```

> ä¸Šé¢æ˜¯æ­£ç¡®çš„é…ç½®æ–‡ä»¶ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªé”™è¯¯çš„é…ç½®æ–‡ä»¶ã€‚ä¼šæŠ›å‡ºå¦‚ä¸‹å¼‚å¸¸ï¼š
>
> The ReplicationController "kubia-rc" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"kubia"}: `selector` does not match template `labels`
>
> `spec.selector` å¿…é¡»å’Œ `spec.template.metadata.labels` ä¸­çš„æ ‡ç­¾å®Œå…¨åŒ¹é…ï¼Œå¦åˆ™å¯åŠ¨çš„æ–°çš„ pod å°†ä¸ä¼šä½¿å¾—å®é™…çš„å‰¯æœ¬æ•°é‡æ¥è¿‘æœŸæœ›çš„å‰¯æœ¬æ•°é‡ã€‚

```yaml
apiVersion: v1
# è¿™é‡Œå®šä¹‰äº† rc
kind: ReplicationController
metadata:
  name: kubia-rc
spec:
  # pod å®ä¾‹æ•°é‡
  replicas: 3
  # selector å†³å®šäº† rc çš„æ“ä½œå¯¹è±¡
  selector:
    app: kubia-rc
  # åˆ›å»ºæ–° pod ä½¿ç”¨çš„æ¨¡æ¿
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
        - name: kubia
          image: luksa/kubia
          ports:
            - containerPort: 8080
```

> æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¸æŒ‡å®š selectorï¼Œè¿™æ ·å®ƒä¼šè‡ªåŠ¨æ ¹æ® pod æ¨¡æ¿ä¸­çš„æ ‡ç­¾è‡ªåŠ¨é…ç½®ï¼Œè¿™ä¹Ÿæ˜¯ kubernetes æ¨èçš„åšæ³•ã€‚

#### 4.2.3 ä½¿ç”¨ ReplactionController

```bash
k get pods
#NAME             READY   STATUS    RESTARTS   AGE
#kubia-rc-54vwv   1/1     Running   0          4m16s
#kubia-rc-j9hxn   1/1     Running   0          4m16s
#kubia-rc-lkmfk   1/1     Running   0          4m16s

# åˆ é™¤ç¬¬ä¸€ä¸ª pod
k delete pod kubia-rc-54vwv
# pod "kubia-rc-54vwv" deleted

# å†æ¬¡æŸ¥çœ‹ï¼Œå‘ç°æœ‰ä¸€ä¸ªæ–°çš„ pod è¢«æ‹‰èµ·äº†
k get pods
#NAME             READY   STATUS    RESTARTS   AGE
#kubia-rc-2l2q5   1/1     Running   0          38s
#kubia-rc-j9hxn   1/1     Running   0          5m27s
#kubia-rc-lkmfk   1/1     Running   0          5m27s
```

#### æ§åˆ¶å™¨å¦‚ä½•åˆ›å»ºæ–° pod

æ§åˆ¶å™¨å¹¶ä¸å¯¹ delete è¡Œä¸ºäº§ç”Ÿä»»ä½•æ“ä½œï¼Œè€Œæ˜¯å› ä¸º delete å¯¼è‡´çš„ pod æ•°é‡ä¸è¶³ï¼Œrc æ¥åˆ›å»ºæ–°çš„ pod ä¿è¯ pod æ•°é‡ã€‚

![åˆ›å»ºæ–°çš„podä»£æ›¿åŸæ¥çš„pod](åˆ›å»ºæ–°çš„podä»£æ›¿åŸæ¥çš„pod.png)

#### åº”å¯¹èŠ‚ç‚¹æ•…éšœ

#### 4.2.4 å°† pod ç§»å…¥æˆ–ç§»å‡º rc çš„ä½œç”¨åŸŸ

> ç”± rc åˆ›å»ºçš„ pod å¹¶ä¸æ˜¯ç»‘å®šåˆ° rc ä¸Šã€‚è€Œæ˜¯ rc ç®¡ç†æ‰€æœ‰ label ä¸ä¹‹å¯¹åº”çš„ podã€‚
>
> éœ€è¦æ³¨æ„çš„é—®é¢˜æ˜¯ï¼Œåªè¦ rc çš„ `selector` èƒ½åŒ¹é…åˆ° podï¼Œå°±è¯´æ˜ pod æ˜¯æ­£ç¡®çš„ã€‚

```bash
k get pod --show-labels
#NAME             READY   STATUS    RESTARTS   AGE   LABELS
#kubia-rc-2l2q5   1/1     Running   0          39m   app=kubia
#kubia-rc-j9hxn   1/1     Running   0          44m   app=kubia
#kubia-rc-lkmfk   1/1     Running   0          44m   app=kubia

# ä¸ºç¬¬ä¸€ä¸ª pod å¢åŠ ä¸€ä¸ªæ–°çš„æ ‡ç­¾
k label po kubia-rc-2l2q5 env=dev

# æŸ¥è¯¢ä¹‹åå‘ç°ï¼Œrc æ²¡æœ‰æ‹‰èµ·æ–°çš„ podï¼Œå› ä¸ºç°åœ¨ rc çš„ selector è¿˜æ˜¯èƒ½åŒ¹é…åˆ°ä¸‰ä¸ª pod çš„
k get pod --show-labels
#NAME             READY   STATUS    RESTARTS   AGE   LABELS
#kubia-rc-2l2q5   1/1     Running   0          39m   app=kubia,env=dev
#kubia-rc-j9hxn   1/1     Running   0          44m   app=kubia
#kubia-rc-lkmfk   1/1     Running   0          44m   app=kubia

# å†æ¬¡ä¿®æ”¹ç¬¬ä¸€ä¸ª
k label po kubia-rc-2l2q5 app=kubia-dev --overwrite

# æŸ¥è¯¢å‘ç° rc æ‹‰èµ·äº†æ–°çš„ podï¼Œå› ä¸ºæ­¤æ—¶ selector å·²ç»åŒ¹é…ä¸åˆ°ä¸‰ä¸ª pod äº†ã€‚
k get pod --show-labels
#NAME             READY   STATUS              RESTARTS   AGE   LABELS
#kubia-rc-2l2q5   1/1     Running             0          42m   app=kubia-dev,env=dev
#kubia-rc-hx8zx   0/1     ContainerCreating   0          2s    app=kubia
#kubia-rc-j9hxn   1/1     Running             0          47m   app=kubia
#kubia-rc-lkmfk   1/1     Running             0          47m   app=kubia
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
  labels:
    app: "kubia"
    gpu: "true"
spec:
  containers:
    - image: luksa/kubia
      name: kubia
```

> å¦‚æœæˆ‘ä»¬ä½¿ç”¨ä¸Šé¢çš„é…ç½®é‡æ–°æ‹‰èµ·ä¸€ä¸ªpodçš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°æ‰¾ä¸åˆ°æ–°çš„ labelsï¼Œå› ä¸ºæ­¤æ—¶podæ•°é‡å·²ç»æ»¡è¶³éœ€æ±‚

```bash
# æŸ¥çœ‹ rc çŠ¶æ€ï¼Œå‘ç°æ–°åˆ›å»ºçš„podå·²ç»è¢«åˆ é™¤
k get pod --show-labels
#NAME             READY   STATUS    RESTARTS   AGE   LABELS
#kubia-rc-2l2q5   1/1     Running   0          91m   app=kubia-dev,env=dev
#kubia-rc-hx8zx   1/1     Running   0          49m   app=kubia
#kubia-rc-j9hxn   1/1     Running   0          96m   app=kubia
#kubia-rc-jht4f   1/1     Running   0          32m   app=kubia

k describe rc kubia-rc
#Events:
#  Type    Reason            Age                 From                    Message
#  ----    ------            ----                ----                    -------
#  Normal  SuccessfulCreate  21m                 replication-controller  Created pod: kubia-rc-hx8zx
#  Normal  SuccessfulCreate  5m36s               replication-controller  Created pod: kubia-rc-jht4f
#  Normal  SuccessfulDelete  7s (x3 over 3m35s)  replication-controller  Deleted pod: kubia-gpu
```

ç°åœ¨ kubia-rc-2l2q5 å·²ç»å®Œå…¨è„±ç¦»äº† rc çš„ç®¡æ§ã€‚

#### 4.2.5 ä¿®æ”¹ pod æ¨¡æ¿

> ä¿®æ”¹ pod æ¨¡æ¿å¹¶ä¸ä¼šå½±å“å·²ç»åˆ›å»ºçš„ podã€‚**è¦ä¿®æ”¹æ—§çš„podï¼Œæˆ‘ä»¬å¾—åˆ é™¤ä»–ä»¬åç­‰ rc æ‹‰èµ·æ–°çš„ pod**

![ä¿®æ”¹ pod æ¨¡æ¿](ä¿®æ”¹ pod æ¨¡æ¿.png)

```bash
# å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç¼–è¾‘ rc
k edit rc kubia-rc
```

#### 4.2.6 æ°´å¹³ç¼©æ”¾ pod

> è°ƒæ•´ spec.replicas å³å¯è¾¾åˆ°æ°´å¹³ç¼©æ”¾ã€‚
>
> kubernetes çš„æ°´å¹³ä¼¸ç¼©æ˜¯ `å£°åå¼` çš„ã€‚

#### ReplicationController æ‰©å®¹

```bash
# æ‰©å®¹
k scale rc kubia-rc --replicas=10

# ç¼©å®¹
k scale rc kubia-rc --replicas=3
```

#### 4.2.7 åˆ é™¤ rc

> åˆ é™¤ rc ä¼šä½¿å¾— pod ä¹Ÿä¼šè¢«åˆ é™¤ï¼Œå¯ä»¥é€šè¿‡å¢åŠ  --cascade=false ä½¿å¾— pod ä¿ç•™

```bash
k delete rc kubia-rc --cascade=false

k get pod --show-labels
#NAME             READY   STATUS    RESTARTS   AGE   LABELS
#kubia-rc-2l2q5   1/1     Running   0          16h   app=kubia-dev,env=dev
#kubia-rc-hx8zx   1/1     Running   0          15h   app=kubia
#kubia-rc-j9hxn   1/1     Running   0          16h   app=kubia
#kubia-rc-jht4f   1/1     Running   0          15h   app=kubia
#kubia-rc-txpcl   1/1     Running   0          42m   app=kubia

# ä½†æ˜¯åˆ é™¤ pod ä¹‹åï¼Œåˆå¯ä»¥é‡å»ºå¹¶é€šè¿‡ label é‡æ–°å…³è”
k create -f kubia-rc.yaml
```

### 4.3 ä½¿ç”¨ ReplicaSet è€Œä¸æ˜¯ ReplicationController

> rs æ˜¯ rc çš„æ›¿ä»£å“ï¼Œé€šå¸¸ä¸ä¼šç›´æ¥åˆ›å»ºå®ƒä»¬ï¼Œè€Œæ˜¯åœ¨åˆ›å»º Deployment æ—¶è‡ªåŠ¨çš„åˆ›å»º rs

#### 4.3.1 æ¯”è¾ƒ rs å’Œ rc

> rs çš„åŒ¹é…èƒ½åŠ›ç›¸å¯¹äº rc è¯´æ›´å¼ºï¼›

#### 4.3.2 å®šä¹‰ rs

```yaml
# æ³¨æ„ï¼Œè¿™é‡Œä¸æ˜¯ v1 çš„ api
# è¿™é‡Œä½¿ç”¨ apiç»„ -> apps
# ä»¥åŠå£°æ˜äº†å®é™…çš„ api ç‰ˆæœ¬ v1beta2
apiVersion: apps/v1beta2
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
        - name: kubia
          image: luksa/kubia
```

```bash
# æ‰§è¡Œ
k create -f kubia-replicaset.yaml
# error: unable to recognize "kubia-replicaset.yaml": no matches for kind "ReplicaSet" in version "apps/v1beta2"

# æŸ¥è¯¢ kubernetes api ç‰ˆæœ¬
k api-versions | grep apps
# apps/v1

# ä¿®æ”¹ apiVersion ç‰ˆæœ¬
k create -f kubia-replicaset.yaml
```

```bash
# æŸ¥è¯¢ pod
k get pod --show-labels
#NAME             READY   STATUS    RESTARTS   AGE    LABELS
#kubia-kk4n4      1/1     Running   0          118s   app=kubia
#kubia-pls4g      1/1     Running   0          118s   app=kubia
#kubia-rc-2l2q5   1/1     Running   0          16h    app=kubia-dev,env=dev
#kubia-rc-c7n88   1/1     Running   0          28m    app=kubia
#kubia-rc-hx8zx   1/1     Running   0          16h    app=kubia
#kubia-rc-j9hxn   1/1     Running   0          16h    app=kubia
#kubia-rc-jht4f   1/1     Running   0          15h    app=kubia
#kubia-rvm2w      1/1     Running   0          118s   app=kubia
```

#### 4.3.4 ä½¿ç”¨ rs çš„æ ‡ç­¾é€‰æ‹©å™¨

- In
- NotIn
- Exists ï¼šä¸æŒ‡å®š values
- DoesNotExist ï¼š ä¸æŒ‡å®š values

```yaml
spec:
  replicas: 3
  selector:
    matchLabels:
      - key: app
        operator: in
        values:
          - kubia
```

### 4.4 ä½¿ç”¨ DaemonSet åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œä¸€ä¸ª pod

> rs å’Œ rc ä¼šåœ¨ kubernetes ä¸Šéƒ¨ç½²ç‰¹å®šæ•°é‡çš„ podã€‚ä½†æ˜¯æœ‰çš„æ—¶å€™æˆ‘ä»¬å¯èƒ½å¸Œæœ›åœ¨é›†ç¾¤çš„æ¯ä¸ªèŠ‚ç‚¹ä¸Šéƒ¨ç½²ä¸€ä¸ª pod å®ä¾‹å°±éœ€è¦ç”¨åˆ° DaemonSet äº†ã€‚

#### 4.4.1 ä½¿ç”¨ DaemonSet

> 1. DaemonSet æ²¡æœ‰æœŸæœ›podæ•°çš„è¯´æ³•ï¼›
> 2. DaemonSet åœ¨æ–°èŠ‚ç‚¹åŠ å…¥èŠ‚ç‚¹æ—¶ï¼Œä¼šè‡ªåŠ¨çš„æ–°åŠ podï¼›

#### 4.4.2 ä½¿ç”¨ DaemonSet åªåœ¨ç‰¹å®šçš„èŠ‚ç‚¹ä¸Šè¿è¡Œ pod

> DaemonSet å°† pod éƒ¨ç½²åˆ°é›†ç¾¤ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹ä¸Šï¼Œé™¤éè¿™äº› pod åªåœ¨éƒ¨åˆ†èŠ‚ç‚¹ä¸Šè¿è¡Œ -- ä¹Ÿå°±æ˜¯è¯´ pod è®¾ç½®äº† nodeSelector å±æ€§ã€‚

#### ç”¨ä¸€ä¸ªä¾‹å­è§£é‡Š DaemonSet

> å‡è®¾ ssd-monitor éœ€è¦åœ¨æ‰€æœ‰ä½¿ç”¨å›ºæ€ç¡¬ç›˜çš„èŠ‚ç‚¹ä¸Šè¿è¡Œã€‚
>
> 
>
> ä¸‹é¢çš„ yaml å£°æ˜äº†ä¸€ä¸ªå¦‚ä¸‹çš„ DaemonSetï¼š
>
> DaemonSet åŒ¹é…åŒ…å«æ ‡ç­¾ `app=ssd-monitor` çš„ podã€‚
>
> template å­˜åœ¨ä¸€ä¸ª NODE_SELECTORï¼Œä¿è¯ pod åªä¼šéƒ¨ç½²åœ¨åŒ…å«æ ‡ç­¾ `disk-ssd` çš„æœºå™¨ä¸Šã€‚

```yaml
apiVersion: apps/v1
# æŒ‡å®šç±»å‹ä¸º DaemonSet
kind: DaemonSet
# DaemonSet çš„åå­—ä¸º ssd-monitor
metadata:
  name: ssd-monitor
spec:
  # DaemonSet ç®¡ç†å’Œ matchLabels åŒ¹é…çš„ pod
  selector:
    matchLabels:
      app: ssd-monitor
  # DaemonSet çš„æ¨¡æ¿
  template:
    # å£°æ˜äº† DaemonSet çš„ label
    metadata:
      labels:
        app: ssd-monitor
    # å£°æ˜äº†é€‰æ‹©å™¨ï¼Œè¡¨ååªä¼šåœ¨åŒ…å« disk=ssd çš„ pod ä¸Š
    spec:
      nodeSelector:
        disk: ssd
      containers:
        - name: main
          image: luksa/ssd-monitor
```

```bash
k get ds
#NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
#ssd-monitor   0         0         0       0            0           disk=ssd        35s

# æ²¡æœ‰è‡ªåŠ¨æ‹‰èµ· pod
k get po
#No resources found in default namespace.

# ä¸ºæœºå™¨æ‰“ä¸Šæ ‡ç­¾
k label nodes minikube disk=ssd

# å¾ˆå¿«å°±å‘ç° pod æ‹‰èµ·äº†
k get po
#NAME                READY   STATUS              RESTARTS   AGE
#ssd-monitor-kvd2k   0/1     ContainerCreating   0          10s
```

### 4.5 è¿è¡Œæ‰§è¡Œå•ä¸ªä»»åŠ¡çš„ pod

> æœŸæœ›ä»»åŠ¡æ‰§è¡Œå®Œä¹‹åå°±é€€å‡ºã€‚

#### 4.5.1 Job èµ„æº

![Jobç®¡ç†çš„podä¼šä¸€ç›´è¢«é‡æ–°å®‰æ’](Jobç®¡ç†çš„podä¼šä¸€ç›´è¢«é‡æ–°å®‰æ’.png)

#### 4.5.2 å®šä¹‰ Job èµ„æº

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      # Job ä¸èƒ½ä½¿ç”¨é»˜è®¤çš„ç­–ç•¥ï¼ˆalwaysï¼‰ï¼Œå› ä¸ºä»–ä»¬ä¸æ˜¯è¦æ— é™æœŸçš„è¿è¡Œ
      restartPolicy: OnFailure
      containers:
        - name: main
          image: luksa/batch-job
```

#### 4.5.3 çœ‹ Job è¿è¡Œä¸€ä¸ª pod

```bash
k create -f exporter.yaml

k get po --show-labels
#NAME                READY   STATUS        RESTARTS   AGE   LABELS
#batch-job-hpkmk     1/1     Running       0          19s   app=batch-job,controller-uid=95704705-974d-413c-a0bf-0d2a5db66dbc,job-name=batch-job

k logs batch-job-hpkmk
#Fri Oct 29 06:52:16 UTC 2021 Batch job starting
#Fri Oct 29 06:54:16 UTC 2021 Finished succesfully
```

#### 4.5.4 åœ¨ Job ä¸­è¿è¡Œå¤šä¸ª pod å®ä¾‹

> ä½œä¸šå¯ä»¥é…ç½®ä¸ºå¤šä¸ª pod å®ä¾‹ï¼Œå¹¶ä»¥å¹¶è¡Œæˆ–è€…ä¸²è¡Œçš„æ–¹å¼è¿è¡Œå®ƒä»¬ã€‚

##### é¡ºåºè¿è¡Œ Job pod

> é€šè¿‡ completions: 5 ä½¿å¾— Job è¿è¡Œå¤šæ¬¡

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: 5
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
        - name: main
          image: luksa/batch-job
```

##### å¹¶è¡Œè¿è¡Œ Job pod

> é€šè¿‡ parallelism: 2 é…ç½®å¹¶è¡Œåº¦ä¸º2

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: 5
  parallelism: 2
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
        - name: main
          image: luksa/batch-job
```

#### 4.5.5 é™åˆ¶ Job pod å®Œæˆä»»åŠ¡çš„æ—¶é—´

> 1. activeDeadlineSeconds å¯ä»¥é™åˆ¶ pod æ—¶é—´ï¼Œé¿å…æ°¸è¿œä¸ç»“æŸ
> 2. è¿˜å¯ä»¥é€šè¿‡ spec.backoffLimit æŒ‡å®šé‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ä¸º 6.

### 4.6 å®‰æ’ Job å®šæœŸè¿è¡Œæˆ–è€…å°†æ¥è¿è¡Œä¸€æ¬¡

```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: batch-job-every-fifteen-minutes
spec:
  # crontab
  schedule: "0,15,30,45 * * * *"
  # pod æœ€è¿Ÿå¿…é¡»åœ¨é¢„å®šæ—¶é—´å15ç§’å¼€å§‹è¿è¡Œï¼Œè¶…è¿‡è¿™ä¸ªæ—¶é—´ä»»åŠ¡å°†è¢«æ ‡è®°ä¸º failed
  startingDeadlineSeconds: 15
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: periodic-batch-job
        spec:
          restartPolicy: OnFailure
          containers:
          - name: main
            image: luksa/batch-job
```

## 5. æœåŠ¡ï¼šè®©å®¢æˆ·ç«¯å‘ç° pod å¹¶ä¸ä¹‹é€šä¿¡

### ç›®å½•

1. åˆ›å»ºæœåŠ¡èµ„æºï¼Œåˆ©ç”¨å•ä¸ªåœ°å€è®¿é—®ä¸€ç»„ podï¼›
2. å‘ç°é›†ç¾¤ä¸­çš„æœåŠ¡ï¼›
3. å°†æœåŠ¡å…¬å¼€ç»™å¤–éƒ¨çš„å®¢æˆ·ç«¯ï¼›
4. ä»é›†ç¾¤å†…éƒ¨è¿æ¥å¤–éƒ¨æœåŠ¡ï¼›
5. æ§åˆ¶ pod ä¸æœåŠ¡å…³è”ï¼›
6. æ’é™¤æœåŠ¡æ•…éšœã€‚

> 1. pod é€šå¸¸éœ€è¦æ¥å—é›†ç¾¤å†…å…¶ä»– pod æˆ–è€…æ¥è‡ªå¤–éƒ¨çš„å®¢æˆ·ç«¯çš„http çš„è¯·æ±‚å¹¶ä½œå‡ºå“åº”ï¼›
> 2. pod çš„ç‰¹ç‚¹
>    1. pod ä¼šéšæ—¶å¯åŠ¨æˆ–è€…å…³é—­ï¼›
>    2. kubernetes åœ¨ pod å¯åŠ¨å‰ä¼šç»™å·²ç»è°ƒåº¦åˆ°èŠ‚ç‚¹ä¸Šçš„ pod åˆ†é… ip åœ°å€ï¼Œå› æ­¤å®¢æˆ·ç«¯ä¸èƒ½æå‰çŸ¥é“ pod çš„åœ°å€ï¼›
>    3. pod çš„æ•°é‡æ˜¯ä¸å›ºå®šçš„ï¼›
> 3. åŸºäº <2>ï¼Œkubernetes æä¾›äº†ä¸€ç§èµ„æºç±»å‹ -- **æœåŠ¡ï¼ˆserviceï¼‰** æ¥è§£å†³ä¸å®¢æˆ·ç«¯æˆ–è€…å…¶ä»– pod é€šä¿¡çš„é—®é¢˜ã€‚

### 5.1 ä»‹ç» service

> service æ˜¯ä¸€ç§ä¸ºä¸€ç»„åŠŸèƒ½ç›¸åŒçš„ pod æä¾›å•ä¸€ä¸å˜çš„æ¥å…¥ç‚¹çš„èµ„æºã€‚
>
> å½“ service å­˜åœ¨æ—¶ï¼Œä»–çš„ ip å’Œ port ä¸ä¼šå˜æ›´ï¼Œå®¢æˆ·ç«¯å¯ä»¥é€šè¿‡è¿™ä¸ª ip å’Œ port è¿æ¥æœåŠ¡è€Œä¸éœ€è¦åœ¨æ„åç«¯ podã€‚

#### ç»“åˆå®ä¾‹è§£é‡ŠæœåŠ¡

> å‡è®¾å­˜åœ¨ä¸€ä¸ªå¦‚ä¸‹æœåŠ¡ï¼š
>
> å®¢æˆ·ç«¯ -> å‰ç«¯ -> DB
>
> é‚£ä¹ˆæˆ‘ä»¬éœ€è¦åšçš„æ˜¯ï¼š
>
> 1. ä¸ºå‰ç«¯ pod åˆ›å»ºæœåŠ¡ï¼Œå¹¶å¯ä»¥åœ¨é›†ç¾¤å¤–éƒ¨è®¿é—®ï¼Œå¯ä»¥æš´éœ²ä¸€ä¸ªå•ä¸€ä¸å˜çš„IPåœ°å€è®©å®¢æˆ·ç«¯è¿æ¥ï¼›
> 2. ä¸ºåç«¯ pod åˆ›å»ºæœåŠ¡ï¼Œå¹¶åˆ†é…ä¸€ä¸ªå›ºå®šçš„ipåœ°å€ï¼Œå°½ç®¡åç«¯ pod ä¼šå˜ï¼Œä½†æ˜¯ service çš„ ip åœ°å€å›ºå®šä¸å˜ã€‚

![å†…éƒ¨å’Œå¤–éƒ¨å®¢æˆ·ç«¯é€šå¸¸é€šè¿‡serviceè¿æ¥åˆ° pod](å†…éƒ¨å’Œå¤–éƒ¨å®¢æˆ·ç«¯é€šå¸¸é€šè¿‡serviceè¿æ¥åˆ° pod.png)

#### 5.1.1 åˆ›å»ºæœåŠ¡

> rc å’Œå…¶ä»–çš„ pod æ§åˆ¶å™¨ä¸­ä½¿ç”¨æ ‡ç­¾é€‰æ‹©å™¨æ¥æŒ‡å®šå“ªäº› pod å±äºåŒä¸€ç»„ã€‚service ä½¿ç”¨ç›¸åŒçš„æœºåˆ¶ã€‚

![serviceé€šè¿‡æ ‡ç­¾é€‰æ‹©å™¨æ¥é€‰æ‹©pod](serviceé€šè¿‡æ ‡ç­¾é€‰æ‹©å™¨æ¥é€‰æ‹©pod.png)

##### é€šè¿‡ kubectl expose åˆ›å»ºæœåŠ¡

> ä¸‹é¢çš„é…ç½®ä¼šç”Ÿæˆ serviceï¼Œservice å°†æ‰€æœ‰æ¥è‡ª 80 ç«¯å£çš„è¯·æ±‚ï¼Œè½¬å‘åˆ°æ‰€æœ‰å…·æœ‰æ ‡ç­¾ `app=kubia` çš„ pod çš„ 8080 ç«¯å£ã€‚

```yaml
apiVersion: v1
# æŒ‡å®šç±»å‹ä¸º service
kind: Service
metadata:
  name: kubia
spec:
  # service å°†è¿æ¥è½¬å‘åˆ°å®¹å™¨çš„ç«¯å£
  ports:
  - port: 80
    targetPort: 8080
  # å…·æœ‰ app=kubia æ ‡ç­¾çš„ pod éƒ½å±äºè¯¥æœåŠ¡
  selector:
    app: kubia
```

```bash
k create -f kubia-svc.yaml
#service/kubia created

k get services --show-labels
#NAME         TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE   LABELS
#kubernetes   ClusterIP      10.96.0.1        <none>        443/TCP          2d    component=apiserver,provider=kubernetes
#kubia        ClusterIP      10.100.127.78    <none>        80/TCP           20s   <none>
```

##### åœ¨è¿è¡Œçš„å®¹å™¨ä¸­è¿œç¨‹æ‰§è¡Œå‘½ä»¤

> `--` ä»£è¡¨ kubectl å‘½ä»¤çš„ç»“æŸã€‚

```bash
# æ‰¾ä¸€å°é›†ç¾¤ä¸­running çš„pod
k get pods
#NAME             READY   STATUS    RESTARTS   AGE
#kubia-rc-44mbb   1/1     Running   0          53s
#kubia-rc-f8fmq   1/1     Running   0          53s
#kubia-rc-fcbwn   1/1     Running   0          53s
#kubia-rc-rm4jl   1/1     Running   0          53s

# æ‰§è¡Œ curl æŒ‡ä»¤
k exec kubia-rc-44mbb -- curl -s http://10.100.127.78:80
#You've hit kubia-rc-fcbwn

k exec kubia-rc-44mbb -- curl -s http://10.100.127.78:80
#You've hit kubia-rc-44mbb

k exec kubia-rc-44mbb -- curl -s http://10.100.127.78:80
#You've hit kubia-rc-rm4jl
```

![kubectl exec æ‰§è¡Œ curl](kubectl exec æ‰§è¡Œ curl.png)

##### é…ç½®serviceä¸Šçš„ä¼šè¯äº²å’Œæ€§

> ç”±äºè´Ÿè½½å‡è¡¡ï¼Œè¯·æ±‚çš„podå¯èƒ½ä¸å›ºå®šã€‚å¦‚æœéœ€è¦è¯·æ±‚æŒ‡å‘åŒä¸€ä¸ªipï¼Œå¯ä»¥é€šè¿‡åˆ¶å®š sessionAffinity å±æ€§ä¸º ClientIPã€‚

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  sessionAffinity: ClientIP
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

```bash
k exec kubia-rc-44mbb -- curl -s http://10.96.8.104:80
#You've hit kubia-rc-rm4jl

k exec kubia-rc-44mbb -- curl -s http://10.96.8.104:80
#You've hit kubia-rc-rm4jl

k exec kubia-rc-44mbb -- curl -s http://10.96.8.104:80
#You've hit kubia-rc-rm4jl
```

##### åŒä¸€ä¸ªæœåŠ¡æš´éœ²å¤šä¸ªç«¯å£

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  sessionAffinity: ClientIP
  ports:
  - port: 80
    name: http
    targetPort: 8080
  - port: 443
    name: https
    targetPort: 8443
  selector:
    app: kubia
```

> ç«¯å£çš„æ ‡ç­¾é€‰æ‹©å™¨åº”ç”¨äºæ•´ä¸ª serviceï¼Œä¸èƒ½å¯¹æ¯ä¸ªç«¯å£åšå•ç‹¬çš„é…ç½®ã€‚

##### ä½¿ç”¨å‘½åçš„ç«¯å£

> åœ¨æœåŠ¡ spec ä¸­ä¹Ÿå¯ä»¥ç»™ä¸åŒçš„ç«¯å£å·å‘½å

```yaml
# åœ¨ pod çš„å®šä¹‰ä¸­æŒ‡å®š port åç§°
apiVersion: v1
kind: Pod
spec:
  containers:
    - name: kubia
      containerPort: 8080
    - name: https
      caontinerPort: 8443
```

```yaml
# åœ¨æœåŠ¡ä¸­å¼•ç”¨å‘½åpod
apiVersion: v1
kind: Service
spec:
  ports:
    - name: http
      port: 80
      targetPort: http
    - name: https
      port: 443
      targetPort: https
```

#### 5.1.2 æœåŠ¡å‘ç°

> kubernetes è¿˜ä¸ºå®¢æˆ·ç«¯æä¾›äº†å‘ç°æœåŠ¡çš„IPå’Œç«¯å£çš„æ–¹å¼ã€‚

##### é€šè¿‡ç¯å¢ƒå˜é‡å‘ç°æœåŠ¡

> ç¯å¢ƒå˜é‡æ˜¯è·å¾—æœåŠ¡IPåœ°å€å’Œç«¯å£çš„ä¸€ç§æ–¹å¼ï¼Œæˆ‘ä»¬è¿˜å…è®¸é€šè¿‡ DNS æ¥è·å¾—æ‰€æœ‰æœåŠ¡çš„IPå’Œåœ°å€

```bash
# æŸ¥çœ‹ç¯å¢ƒå˜é‡
k exec kubia-rc-44mbb -- env
#...
# æœåŠ¡çš„é›†ç¾¤ ip å’Œ port
#KUBIA_SERVICE_HOST=10.100.127.78
#KUBIA_SERVICE_PORT=80
#...
```

##### é€šè¿‡DNSå‘ç°æœåŠ¡

```bash
#coredns æ˜¯ kubernetes å†…éƒ¨çš„ DNS æœåŠ¡
k get pod --show-labels --namespace kube-system
#coredns-74ff55c5b-klnsq            1/1     Running   1          5d    k8s-app=kube-dns,pod-template-hash=74ff55c5b
```

##### é€šè¿‡FQDN(Fully Qualified Domain Name)è¿æ¥æœåŠ¡

> åœ¨æˆ‘ä»¬å‰é¢çš„ä¾‹å­ä¸­ï¼Œå‰ç«¯podå¯ä»¥é€šè¿‡ `backend-database.default.svc.cluter.local` è®¿é—®åç«¯æ•°æ®æœåŠ¡
>
> - backend-database å¯¹åº”äºæœåŠ¡åç§°
> - default è¡¨ç¤ºæœåŠ¡çš„å‘½åç©ºé—´
> - svc.cluster.local æ˜¯åœ¨æ‰€æœ‰é›†ç¾¤æœ¬åœ°æœåŠ¡åä¸­ä½¿ç”¨çš„å¯é…ç½®é›†ç¾¤åŸŸåç¼€
>
> å¦‚æœå‰ç«¯podå’Œæ•°æ®åº“podåœ¨åŒä¸€ä¸ªå‘½åç©ºé—´ä¸‹ï¼Œå¯ä»¥çœç•¥ svc.cluster.local åç¼€ï¼Œç”šè‡³å‘½åç©ºé—´ã€‚

##### åœ¨ pod å®¹å™¨ä¸­è¿è¡Œ shell

```bash
# è¿›å…¥ bash
k exec kubia-rc-44mbb -it -- /bin/bash

curl http://kubia.default.svc.cluster.local
#You've hit kubia-rc-rm4jl

curl http://kubia.default
#You've hit kubia-rc-rm4jl

curl http://kubia
#You've hit kubia-rc-rm4jl

cat /etc/resolv.conf
#nameserver 10.96.0.10
#search default.svc.cluster.local svc.cluster.local cluster.local
#options ndots:5
```

##### æ— æ³•pingé€šæœåŠ¡IPçš„åŸå› 

> æœåŠ¡çš„é›†ç¾¤IPæ˜¯ä¸€ä¸ªè™šæ‹ŸIPï¼Œå¹¶ä¸”åªæœ‰å’ŒæœåŠ¡ç«¯å£ç»“åˆæ—¶æ‰æœ‰æ„ä¹‰ã€‚

### 5.2 è¿æ¥é›†ç¾¤å¤–éƒ¨çš„æœåŠ¡

#### 5.2.1 ä»‹ç»æœåŠ¡ endpoint

> æœåŠ¡å¹¶ä¸æ˜¯å’Œpodç›´æ¥ç›¸è¿çš„ï¼Œæœ‰ä¸€ç§èµ„æºä»‹äºä¸¤è€…ä¹‹é—´ -- endpointã€‚

```bash
k describe services kubia
#...
#Selector:          app=kubia
#Endpoints:         10.244.1.10:8080,10.244.2.19:8080,10.244.2.20:8080 + 1 more...
#...

k get endpoints kubia
#NAME    ENDPOINTS                                                        AGE
#kubia   10.244.1.10:8443,10.244.2.19:8443,10.244.2.20:8443 + 5 more...   2d22h

k describe endpoints kubia
```

#### 5.2.2 æ‰‹åŠ¨é…ç½®æœåŠ¡çš„ endpoint

> å¦‚æœåˆ›å»ºäº†ä¸åŒ…å« `selector` çš„ serviceï¼Œkubernetes å°†ä¸ä¼šåˆ›å»º endpoint èµ„æºï¼Œå› ä¸ºç¼ºå°‘é€‰æ‹©å™¨ï¼Œå°†æ— æ³•ç¡®å®š service ä¸­åŒ…å«äº†å“ªäº› podã€‚

##### åˆ›å»ºæ²¡æœ‰é€‰æ‹©å™¨çš„æœåŠ¡

> å®šä¹‰ä¸€ä¸ªåä¸º external-service çš„æœåŠ¡ï¼Œæ¥æ”¶ç«¯å£ 80 ä¸Šçš„è¿æ¥ï¼Œå¹¶æ²¡æœ‰ä¸ºæœåŠ¡é€‰å®šä¸€ä¸ª pod selector

```yaml
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  ports:
    - port: 80
```

##### ä¸ºæ²¡æœ‰é€‰æ‹©å™¨çš„æœåŠ¡åˆ›å»º endpoint èµ„æº

> è¿™æ ·ï¼Œä¸Šé¢æ²¡æœ‰ pod é€‰æ‹©å™¨çš„ service å°±å¯ä»¥è¿æ¥åˆ°ä¸‹é¢çš„è¿™äº› endpoint äº†ã€‚

```yaml
apiVersion: v1
kind: Endpoints
# endpoint çš„åç§°å¿…é¡»å’ŒæœåŠ¡çš„åç§°ç›¸åŒ¹é…
metadata:
  name: external-service
subsets:
  - addresses:
    - ip: 11.11.11.11
    - ip: 22.22.22.22
    ports:
    - port: 80
```

#### 5.2.3 ä¸ºå¤–éƒ¨æœåŠ¡åˆ›å»ºåˆ«å

##### åˆ›å»º ExternalName ç±»å‹çš„æœåŠ¡

```yaml
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  type: ExternalName
  externalName: someapi.somecompany.com
  ports:
    - port: 80
```

### 5.3 å°†æœåŠ¡æš´éœ²ç»™å¤–éƒ¨å®¢æˆ·ç«¯

- å°†æœåŠ¡ç±»å‹è®¾ç½®ä¸º NodePortï¼Œå¹¶å°†åœ¨è¯¥ç«¯å£ä¸Šæ¥æ”¶åˆ°çš„æµé‡é‡å®šå‘åˆ°åŸºç¡€æœåŠ¡ï¼›
- å°†æœåŠ¡çš„ç±»å‹è®¾ç½®æˆ LoadBalanceï¼Œä¸€ç§ NodePort çš„æ‰©å±•ç±»å‹ï¼›
- åˆ›å»ºä¸€ä¸ª ingress èµ„æºã€‚

#### 5.3.1 ä½¿ç”¨ NodePort ç±»å‹çš„æœåŠ¡

##### åˆ›å»º NodePort ç±»å‹çš„æœåŠ¡

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30123
  selector:
    app: kubia
```

```bash
# æ‰“å¼€ minikube çš„å¤–éƒ¨è®¿é—®é€šé“
minikube service kubia-nodeport --url

#ğŸƒ  Starting tunnel for service kubia-nodeport.
#|-----------|----------------|-------------|------------------------|
#| NAMESPACE |      NAME      | TARGET PORT |          URL           |
#|-----------|----------------|-------------|------------------------|
#| default   | kubia-nodeport |             | http://127.0.0.1:60965 |
#|-----------|----------------|-------------|------------------------|
#http://127.0.0.1:60965
#â—  Because you are using a Docker driver on darwin, the terminal needs to be open to run it.

curl http://127.0.0.1:60965
```

![å¤–éƒ¨å®¢æˆ·ç«¯é€šè¿‡èŠ‚ç‚¹1æˆ–è€…èŠ‚ç‚¹2è¿æ¥åˆ°NodePortæœåŠ¡](å¤–éƒ¨å®¢æˆ·ç«¯é€šè¿‡èŠ‚ç‚¹1æˆ–è€…èŠ‚ç‚¹2è¿æ¥åˆ°NodePortæœåŠ¡.png)

#### 5.3.2 é€šè¿‡è´Ÿè½½å‡è¡¡å™¨å°†æœåŠ¡æš´éœ²å‡ºæ¥

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

```bash
# å¯åŠ¨ minikube url
minikube service kubia-loadbalancer --url
```

##### SessionAffinity

æˆ‘ä»¬å¯ä»¥é€šè¿‡æµè§ˆå™¨å’Œ curl è®¿é—®æœåŠ¡ï¼Œä½†æ˜¯æˆ‘ä»¬å‘ç°ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼šæµè§ˆå™¨æ¯æ¬¡éƒ½æ˜¯åŒä¸€ä¸ªpodï¼Œè€Œ curl åˆ™ä¸ä¸€å®šï¼Œæ˜¯å¦æ˜¯å› ä¸ºè®¾ç½®äº† sessionAffinity å‘¢ï¼Ÿ

ç»“è®ºæ˜¯ä¸æ˜¯ï¼Œæ˜¯å› ä¸ºæµè§ˆå™¨ä½¿ç”¨ keep-alive è¿æ¥ï¼Œå¹¶é€šè¿‡å•ä¸ªè¿æ¥å‘é€æ‰€æœ‰è¯·æ±‚ã€‚è€Œ curl æ¯æ¬¡éƒ½ä¼šæ‰“å¼€ä¸€ä¸ªæ–°çš„è¿æ¥ã€‚

**æœåŠ¡åœ¨è¿æ¥çº§åˆ«å·¥ä½œ**ï¼Œæ‰€ä»¥ä¸ç®¡æ˜¯å¦è®¾ç½® sessionAffinityï¼Œç”¨æˆ·åœ¨æµè§ˆå™¨ä¸­å§‹ç»ˆä¼šä½¿ç”¨ç›¸åŒçš„è¿æ¥ã€‚

![å¤–éƒ¨å®¢æˆ·ç«¯è¿æ¥ä¸€ä¸ªLoadBalanceræœåŠ¡](å¤–éƒ¨å®¢æˆ·ç«¯è¿æ¥ä¸€ä¸ªLoadBalanceræœåŠ¡.png)

#### 5.3.3 äº†è§£å¤–éƒ¨è¿æ¥çš„ç‰¹æ€§

##### äº†è§£å¹¶é˜²æ­¢ä¸å¿…è¦çš„ç½‘ç»œè·³æ•°

> å®¢æˆ·ç«¯ -> LoadBalancer -> Service è¿™ä¸ªé“¾è·¯ä¸­ï¼Œ LoadBalancer å’Œ Service å¯èƒ½åœ¨ä¸¤ä¸ªä¸åŒçš„èŠ‚ç‚¹ã€‚
>
> æˆ‘ä»¬å¯ä»¥é…ç½®ä»…ä»…é‡å®šå‘åˆ°åŒèŠ‚ç‚¹çš„ podã€‚
>
> spec.externalTrafficPolicy

```bash
k explain service.spec.externalTrafficPolicy

#KIND:     Service
#VERSION:  v1
#
#FIELD:    externalTrafficPolicy <string>
#
#DESCRIPTION:
#     externalTrafficPolicy denotes if this Service desires to route external
#     traffic to node-local or cluster-wide endpoints. "Local" preserves the
#     client source IP and avoids a second hop for LoadBalancer and Nodeport type
#     services, but risks potentially imbalanced traffic spreading. "Cluster"
#     obscures the client source IP and may cause a second hop to another node,
#     but should have good overall load-spreading.
```

### 5.4 é€šè¿‡ ingress æš´éœ²æœåŠ¡

#### ä¸ºä»€ä¹ˆéœ€è¦ ingress

> æ¯ä¸ª LoadBalancer éƒ½éœ€è¦è‡ªå·±çš„è´Ÿè½½å‡è¡¡å™¨ï¼Œä½†æ˜¯ ingress å¯ä»¥ä¸ºå¤šä¸ªæœåŠ¡æä¾›è®¿é—®ã€‚

![ingress](é€šè¿‡ä¸€ä¸ªingressæš´éœ²å¤šä¸ªæœåŠ¡.png)

```bash
#æŸ¥çœ‹ ingress
minikube addons list

#å¼€å¯ ingress
minikube addons enable ingress
```

#### 5.4.1 åˆ›å»º ingress èµ„æº

> æœ€å¼€å§‹ï¼Œæˆ‘é…ç½®äº† `serviceName: kubia-nodexport` ä½†æ˜¯æ²¡æœ‰å¯åŠ¨ `kubia-nodeexport`ï¼Œæ‰€ä»¥ä¸€ç›´ 502.
>
> é…ç½® `/etc/host`ä½¿å¾— kubia.example.com -> è™šæ‹Ÿip

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com
    http:
      paths:
        - path: /
          backend:
            serviceName: kubia
            servicePort: 80
```

```bash
k get ingress
#NAME    CLASS    HOSTS               ADDRESS          PORTS   AGE   LABELS
#kubia   <none>   kubia.example.com   192.168.99.102   80      16s   <none>

curl http://kubia.example.com
#You've hit kubia-rc-xxh27

curl 192.168.99.102
# 404 å¼‚å¸¸
```

##### äº†è§£ ingress çš„å·¥ä½œåŸç†

> ingress æ§åˆ¶å™¨é€šè¿‡ http è¯·æ±‚çš„ header ç¡®å®šå®¢æˆ·ç«¯å°è¯•è®¿é—®å“ªä¸ª serviceï¼Œ**é€šè¿‡ä¸è¯¥æœåŠ¡å…³è”çš„ endpoint å¯¹è±¡æŸ¥çœ‹ podId**ã€‚
>
> ingress æ§åˆ¶å™¨ä¸ä¼šæŠŠè¯·æ±‚è½¬å‘ç»™æœåŠ¡ï¼Œåªç”¨å®ƒæ¥é€‰æ‹©ä¸€ä¸ª podã€‚

![é€šè¿‡ingressè®¿é—®pod](é€šè¿‡ingressè®¿é—®pod.png)

#### 5.4.3 é€šè¿‡ç›¸åŒçš„ ingress æš´éœ²å¤šä¸ªæœåŠ¡

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com
    http:
      paths:
        #        - path: /
        #          backend:
        #            serviceName: kubia
        #            servicePort: 80
        - path: /kubia
          backend:
            serviceName: kubia
            servicePort: 80
```

#### 5.4.4 é…ç½® ingress å¤„ç† TLS ä¼ è¾“

```bash
#åˆ›å»ºç§é’¥å’Œè¯ä¹¦
openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj
openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj /CN=kubia.example.com

#åˆ›å»º Secret
#ç§é’¥å’Œè¯ä¹¦ç°åœ¨å­˜å‚¨åœ¨åä¸º tls-secret çš„ Secret ä¸­ã€‚
kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key
```

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia-tls
spec:
  # é…ç½® tls
  tls:
  - hosts:
    # æ¥å—æ¥è‡ª kubia.example.com ä¸»æœºçš„ tls è¿æ¥
    - kubia.example.com
    # ä» tls-secret ä¸­è·å¾—ä¹‹å‰åˆ›å»ºçš„ç§é’¥å’Œè¯ä¹¦
    secretName: tls-secret
  rules:
  - host: kubia.example.com
    http:
      paths:
        #        - path: /
        #          backend:
        #            serviceName: kubia
        #            servicePort: 80
        - path: /kubia-tls
          backend:
            serviceName: kubia
            servicePort: 80
```

```bash
#è®¿é—® tls æœåŠ¡
curl -k -v https://kubia.example.com/kubia-tls
#...
#You've hit kubia-rc-c7ngq
#* Connection #0 to host kubia.example.com left intact
#* Closing connection 0
```

### 5.5 pod å°±ç»ªåå‘å‡ºä¿¡å·

#### 5.5.1 å°±ç»ªæ¢é’ˆ

> å’Œå­˜æ´»æ¢é’ˆä¸€æ ·ï¼Œå°±ç»ªæ¢é’ˆæœ‰ä¸‰ç§ç±»å‹ï¼š
>
> 1. exec æ¢é’ˆ
> 2. HTTP GET æ¢é’ˆ
> 3. TCP socket æ¢é’ˆ

![å°±ç»ªæ¢é’ˆæ¢æµ‹endpoint](å°±ç»ªæ¢é’ˆæ¢æµ‹endpoint.png)

##### æ·»åŠ å°±ç»ªæ¢é’ˆ

> ä¸‹é¢çš„é…ç½®æ–‡ä»¶ï¼Œå› ä¸ºåˆå§‹æ²¡æœ‰ `/var/ready` æ–‡ä»¶ï¼Œæ‰€ä»¥ pod çš„çŠ¶æ€ä¸€ç›´æ˜¯é”™çš„ã€‚åˆ›å»º `/var/ready` æ–‡ä»¶

```yaml
apiVersion: v1
# è¿™é‡Œå®šä¹‰äº† rc
kind: ReplicationController
metadata:
  name: kubia-rc-readiness-probe
spec:
  # pod å®ä¾‹æ•°é‡
  replicas: 1
  # selector å†³å®šäº† rc çš„æ“ä½œå¯¹è±¡
  selector:
    app: kubia
  # åˆ›å»ºæ–° pod ä½¿ç”¨çš„æ¨¡æ¿
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
        - name: kubia
          image: luksa/kubia
          ports:
            - containerPort: 8080
          # pod ä¸­çš„æ¯ä¸ªå®¹å™¨éƒ½ä¼šæœ‰ä¸€ä¸ªå°±ç»ªæ¢é’ˆ
          readinessProbe:
            exec:
              command:
                - ls
                - /var/ready
```

```bash
k get pods --show-labels
#kubia-rc-readiness-probe-r6nxn   0/1     Running   0          2m23s   app=kubia

k exec kubia-rc-readiness-probe-r6nxn -it -- /bin/bash

touch /var/ready
```

### 5.6 ä½¿ç”¨ headless æœåŠ¡æ¥å‘ç°ç‹¬ç«‹çš„ pod

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-headless
spec:
  # ä½¿å¾—æœåŠ¡æˆä¸º headless æœåŠ¡
  clusterIP: None
  ports:
  - port: 80
    name: http
    targetPort: 8080
  selector:
    app: kubia
```

```bash
# kubia-headless æ²¡æœ‰ ClusterIP
k get service --show-labels
#NAME                 TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE     LABELS
#kubia-headless       ClusterIP      None             <none>        80/TCP           20s     <none>
#kubia-loadbalancer   LoadBalancer   10.107.136.228   <pending>     80:30062/TCP     3h33m   <none>
```

### 5.7 æ’é™¤æœåŠ¡æ•…éšœ

1. åŒºåˆ†é›†ç¾¤å†…IPå’Œé›†ç¾¤å¤–IPï¼›
2. ä¸é€šè¿‡ ping æ¥æ¢æµ‹æœåŠ¡ï¼›
3. å°±ç»ªæ¢é’ˆ/å­˜æ´»æ¢é’ˆä¸èƒ½å‡ºç°é”™è¯¯ï¼›
4. è¦ç¡®è®¤æŸä¸ªå®¹å™¨æ˜¯æœåŠ¡çš„ä¸€éƒ¨åˆ†ï¼Œå¯ä»¥é€šè¿‡ `kubectl get endpoints` æ¥æ£€æŸ¥ç›¸åº”çš„ç«¯ç‚¹å¯¹è±¡ï¼›
5. å½“ FQDN ä¸èµ·ä½œç”¨æ—¶ï¼Œå¯ä»¥å°è¯•ä¸€ä¸‹ä½¿ç”¨IPè®¿é—®æœåŠ¡ï¼›
6. å°è¯•ç›´æ¥è¿æ¥åˆ°PodIdç¡®è®¤podæ­£å¸¸å·¥ä½œï¼›

##### ç¡®è®¤å®¹å™¨æ˜¯æœåŠ¡çš„ä¸€éƒ¨åˆ†

```bash
k get pods -o wide
#NAME                             READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
#kubia-rc-c7ngq                   1/1     Running   0          3h52m   10.244.1.3   minikube-m02   <none>           <none>
#kubia-rc-ctjn9                   1/1     Running   0          3h52m   10.244.2.4   minikube-m03   <none>           <none>
#kubia-rc-readiness-probe-r6nxn   1/1     Running   0          36m     10.244.1.7   minikube-m02   <none>           <none>
#kubia-rc-xmb2w                   1/1     Running   0          3h52m   10.244.2.5   minikube-m03   <none>           <none>
#kubia-rc-xxh27                   1/1     Running   0          3h52m   10.244.1.4   minikube-m02   <none>           <none>

k get endpoints
#NAME                 ENDPOINTS                                                     AGE
#external-service     11.11.11.11:80,22.22.22.22:80                                 8m40s
#kubernetes           192.168.99.100:8443                                           4h
#kubia                10.244.1.3:8443,10.244.1.4:8443,10.244.1.7:8443 + 7 more...   3h54m
#kubia-headless       10.244.1.3:8080,10.244.1.4:8080,10.244.1.7:8080 + 2 more...   21m
#kubia-loadbalancer   10.244.1.3:8080,10.244.1.4:8080,10.244.1.7:8080 + 2 more...   3h54m

k describe endpoints kubia-headless
#Name:         kubia-headless
#Namespace:    default
#Labels:       service.kubernetes.io/headless=
#Annotations:  endpoints.kubernetes.io/last-change-trigger-time: 2021-11-02T07:38:47Z
#Subsets:
#  Addresses:          10.244.1.3,10.244.1.4,10.244.1.7,10.244.2.4,10.244.2.5
#  NotReadyAddresses:  <none>
#  Ports:
#    Name  Port  Protocol
#    ----  ----  --------
#    http  8080  TCP
#
#Events:  <none>
```

## 6. å·ï¼šå°†ç£ç›˜æŒ‚è½½åˆ°å®¹å™¨ä¸Š

> 1. æˆ‘ä»¬å¯èƒ½ä¸å¸Œæœ› pod çš„æ•´ä¸ªæ–‡ä»¶ç³»ç»Ÿè¢«æŒä¹…åŒ–ï¼Œåˆå¸Œæœ›å®ƒèƒ½ä¿å­˜å®é™…æ•°æ®ï¼Œä¸ºæ­¤ kubernetes æä¾›äº† `å·`ï¼›
> 2. kubernetes ä¸­å·æ˜¯ pod çš„ä¸€éƒ¨åˆ†ï¼Œå’Œ pod çš„ç”Ÿå‘½å‘¨æœŸä¸€æ · -- åœ¨å¯åŠ¨æ—¶åˆ›å»ºï¼Œåœ¨ delete æ—¶é”€æ¯ï¼›

### 6.1 ä»‹ç»å·

#### 6.1.1 å·çš„åº”ç”¨ç¤ºä¾‹

> å‡è®¾å­˜åœ¨ä¸¤ä¸ªå· `publicHtml` å’Œ `logVol`ã€‚
>
> /var/htdocs -> publicHtml
>
> /var/logs -> logVol
>
> /var/html -> publicHtml
>
> /var/logs -> logVol
>
> è¿™æ ·ä¸‰ä¸ªå®¹å™¨å°±å¯ä»¥å…±äº«äº†æ•°æ®äº†ã€‚

![ä¸‰ä¸ªå®¹å™¨å…±äº«æŒ‚åœ¨åœ¨ä¸åŒçš„å®‰è£…è·¯å¾„çš„ä¸¤ä¸ªå·ä¸Š](ä¸‰ä¸ªå®¹å™¨å…±äº«æŒ‚åœ¨åœ¨ä¸åŒçš„å®‰è£…è·¯å¾„çš„ä¸¤ä¸ªå·ä¸Š.png)

#### 6.1.2 ä»‹ç»å¯ç”¨çš„å·ç±»å‹

- emptyDir å­˜å‚¨ä¸´æ—¶æ•°æ®çš„ç®€å•ç©ºç›®å½•
- hostPath ç”¨äºå°†ç›®å½•ä»å·¥ä½œèŠ‚ç‚¹çš„æ–‡ä»¶ç³»ç»ŸæŒ‚åœ¨åˆ°podä¸Š
- gitRepo é€šè¿‡æ£€å‡º git ä»“åº“çš„å†…å®¹æ¥åˆå§‹åŒ–çš„å·
- Nfs æ€ªå“‰åˆ° pod ä¸­çš„ NFS å…±äº«å·
- ...

### 6.2 é€šè¿‡å·åœ¨å®¹å™¨ä¹‹é—´å…±äº«æ•°æ®

#### 6.2.1 ä½¿ç”¨ emptyDir å·

##### åœ¨ pod ä¸­ä½¿ç”¨ emptyDir å·

> æŠŠä¸Šé¢çš„ä¾‹å­ç»§ç»­ç®€åŒ–ï¼Œåªä¿ç•™ WebServer å’Œ ContentAgentã€‚
>
> æˆ‘ä»¬ä½¿ç”¨ nginx ä½œä¸º web æœåŠ¡å™¨å’Œ UNIX fortune å‘½ä»¤æ¥ç”Ÿæˆ html å†…å®¹ã€‚

```dockerfile
FROM ubuntu:latest

RUN apt-get update ; apt-get -y install fortune
ADD fortuneloop.sh /bin/fortuneloop.sh

ENTRYPOINT /bin/fortuneloop.sh
```

```bash
#!/bin/bash
trap "exit" SIGINT
mkdir /var/htdocs

while :
do
  echo $(date) Writing fortune to /var/htdocs/index.html
  /usr/games/fortune > /var/htdocs/index.html
  sleep 10
done
```

##### åˆ›å»ºpod

> ä¸‹é¢çš„é…ç½®åˆ›å»ºäº†ä¸¤ä¸ªå®¹å™¨ï¼šhtml-generator å’Œ web-serverã€‚
>
> html-generator æ¯10sè¾“å‡ºæ•°æ®åˆ° `/var/htdocs/index.html` ä¸­ï¼Œè€Œ `/var/htdocs` è¿™ä¸ªæ–‡ä»¶è¢«æŒ‚è½½åˆ°äº†å· html ä¸‹ã€‚
>
> web-server ä» `/user/share/nginx/html` ä¸‹è¯»å–æ•°æ®ï¼Œè€Œè¿™ä¸ªæ–‡ä»¶ä¹Ÿè¢«æŒ‚è½½åˆ°äº†å· html ä¸‹ã€‚

```yaml
apiVersion: v1
#åˆ›å»ºä¸€ä¸ªpod
kind: Pod
#Pod åæ˜¯ fortune
metadata:
  name: fortune
spec:
  containers:
  - image: luksa/fortune
    #å®¹å™¨åæ˜¯html-generator
    name: html-generator
    #åä¸ºhtmlçš„å·æŒ‚è½½åœ¨å®¹å™¨/var/htdocsä¸­
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    #å®¹å™¨åæ˜¯web-server
    name: web-server
    volumeMounts:
    #åä¸ºhtmlçš„å·æŒ‚è½½åœ¨å®¹å™¨/user/share/nginx/htmlä¸­
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  #ä¸€ä¸ªåä¸ºhtmlçš„å•ç‹¬emptyDirå·ã€‚
  volumes:
  - name: html
    emptyDir: {}
```

##### æŒ‡å®šç”¨äº EMPTYDIR çš„ä»‹è´¨

```yaml
#æŒ‡å®šåŸºäº tmfs æ–‡ä»¶ç³»ç»Ÿï¼ˆåŸºäºå†…å­˜è€Œéç¡¬ç›˜ï¼‰åˆ›å»ºã€‚
  volumes:
  - name: html
    emptyDir: 
      medium: Memory
```

#### 6.2.2 ä½¿ç”¨ git ä»“åº“ä½œä¸º volumn

> gitRepo çš„æŒ‚è½½åœ¨ git å¤åˆ¶ä¹‹åï¼Œå®¹å™¨å¯åŠ¨ä¹‹å‰ã€‚æ‰€ä»¥git çš„æ›´æ–°åœ¨ rc é‡å¯ pod æ—¶ç”Ÿæ•ˆã€‚

![gitRepo](gitRepo.png)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gitrepo-volume-pod
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    #æŒ‡å®šå®¹å™¨æš´éœ²çš„åè®®
    ports:
    - containerPort: 80
      protocol: TCP
  #å£°æ˜ä¸€ä¸ªåä¸º html çš„ volumnï¼Œè¿™ä¸ª volumn æ˜¯ä¸€ä¸ª gitRepo
  volumes:
  - name: html
    gitRepo:
      repository: https://github.com/luksa/kubia-website-example.git
      revision: master
      #æŒ‡å®šå½“å‰ç›®å½•ä¸ºgitè·¯å¾„çš„æ ¹ç›®å½•ï¼Œä¸æŒ‡å®šçš„è¯å°†ä¼šå­˜åœ¨ä¸€ä¸ª kubia-website-example çš„æ–‡ä»¶å¤¹
      directory: .
```

```bash
k port-forward gitrepo-volume-pod 8080:80

curl http://localhost:8080/
#<html>
#<body>
#Hello there.
#</body>
#</html>
```

##### ä»‹ç» sidecar å®¹å™¨

> **å¦‚æœæˆ‘ä»¬å¸Œæœ›æ—¶åˆ»ä¿æŒ gitRepo å’Œ git ä»£ç ä¸€è‡´ï¼Œæˆ‘ä»¬é€šè¿‡å¢åŠ ä¸€ä¸ª sidecar container æ¥å®ç°ã€‚**
>
> git åŒæ­¥è¿›ç¨‹ä¸åº”è¯¥è¿è¡Œåœ¨ä¸ nginx ç›¸åŒçš„å®¹å™¨ä¸­ï¼Œè€Œæ˜¯åœ¨ç¬¬äºŒä¸ªå®¹å™¨ -- **sidecar container**ã€‚
>
> å®ƒæ˜¯ä¸€ç§å®¹å™¨ï¼Œå¢åŠ äº†å¯¹ pod ä¸»å®¹å™¨çš„æ“ä½œã€‚å¯ä»¥å°†ä¸€ä¸ª sidecar æ·»åŠ åˆ°ä¸€ä¸ª pod ä¸­ï¼Œè¿™æ ·å°±å¯ä»¥ä½¿ç”¨ç°æœ‰çš„å®¹å™¨é•œåƒï¼Œè€Œä¸æ˜¯å°†é™„åŠ é€»è¾‘å¡«å…¥ä¸»åº”ç”¨ç¨‹åºçš„ä»£ç ä¸­ï¼Œè¿™ä¼šå¯¼è‡´å®ƒè¿‡äºå¤æ‚å’Œä¸å¯ç”¨ã€‚

### 6.3 è®¿é—®å·¥ä½œèŠ‚ç‚¹æ–‡ä»¶ç³»ç»Ÿä¸Šçš„æ–‡ä»¶

> ä¸€èˆ¬ pod ä¸åº”è¯¥è®¿é—® node çš„ç›®å½•ï¼Œå› ä¸ºè¿™ä¼šå¯¼è‡´ pod å’Œ node ç»‘å®šã€‚

#### 6.3.1 ä»‹ç» hostPath å·

![hostPathå·å°†å·¥ä½œèŠ‚ç‚¹ä¸Šçš„æ–‡ä»¶æˆ–ç›®å½•æŒ‚åœ¨åˆ°å®¹å™¨çš„æ–‡ä»¶ç³»ç»Ÿä¸­](hostPathå·å°†å·¥ä½œèŠ‚ç‚¹ä¸Šçš„æ–‡ä»¶æˆ–ç›®å½•æŒ‚åœ¨åˆ°å®¹å™¨çš„æ–‡ä»¶ç³»ç»Ÿä¸­.png)

#### 6.3.2 æ£€æŸ¥ä½¿ç”¨ hostPath å·çš„ç³»ç»Ÿ pod

```bash
k describe po kindnet-bgpx8 --namespace kube-system

#Volumes:
#  cni-cfg:
#    Type:          HostPath (bare host directory volume)
#    Path:          /etc/cni/net.mk
#    HostPathType:  DirectoryOrCreate
#  xtables-lock:
#    Type:          HostPath (bare host directory volume)
#    Path:          /run/xtables.lock
#    HostPathType:  FileOrCreate
#  lib-modules:
#    Type:          HostPath (bare host directory volume)
#    Path:          /lib/modules
#    HostPathType:
#  kindnet-token-mtmvl:
#    Type:        Secret (a volume populated by a Secret)
#    SecretName:  kindnet-token-mtmvl
#    Optional:    false1-(11-03 09:36:18','1450004069','0','0','ozEm3uFeSXWxa0h6t2PVqFw09_Hs','398','13','1','95874014','398032001','1','10','2','10','100','0')
#
```

### 6.4 ä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨

> 1. ä¸ºäº†ä¿è¯ pod ä¸å’Œ node ç»‘å®šï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ NAS ä¿è¯æ¯ä¸ª pod éƒ½å¯ä»¥è®¿é—®æˆ‘ä»¬çš„æŒä¹…åŒ–å­˜å‚¨ã€‚
> 2. å› ä¸ºæˆ‘ä»¬ä½¿ç”¨ gcePersistentDiskï¼Œä¸‹é¢çš„ pod æ˜¯æ— æ³•æ­£å¸¸æ‹‰èµ·çš„ã€‚

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mongodb 
spec:
  #å£°æ˜ä¸€ä¸ªåå­—ä¸ºmongodb-dataï¼Œç±»å‹ä¸ºgcePersistentDisk
  #gcePersistentDisk çš„ PD resource ç±»å‹æ˜¯ mongondbï¼Œä½¿ç”¨çš„æ–‡ä»¶ç³»ç»Ÿæ˜¯ ext4
  volumes:
  - name: mongodb-data
    gcePersistentDisk:
      pdName: mongodb
      fsType: ext4
  containers:
  - image: mongo
    name: mongodb
    #å°†mongodbçš„é•œåƒ mount åˆ° /data/db
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
```

![å¸¦æœ‰å•ä¸ªè¿è¡Œmongodbçš„å®¹å™¨çš„pod](å¸¦æœ‰å•ä¸ªè¿è¡Œmongodbçš„å®¹å™¨çš„pod.png)

### 6.5 ä»åº•å±‚å­˜å‚¨æŠ€æœ¯è§£è€¦ pod

#### 6.5.1 ä»‹ç»æŒä¹…å·å’ŒæŒä¹…å·å£°æ˜

- PersistentVolume
- PersistentVolumeClaimï¼ŒæŒ‡å®šæœ€ä½å®¹é‡è¦æ±‚å’Œè®¿é—®æ¨¡å¼

![æŒä¹…å·ç”±é›†ç¾¤ç®¡ç†å‘˜æä¾›ï¼Œå†°æ¯podé€šè¿‡æŒä¹…å·å£°æ˜æ¥æ¶ˆè´¹](æŒä¹…å·ç”±é›†ç¾¤ç®¡ç†å‘˜æä¾›ï¼Œå†°æ¯podé€šè¿‡æŒä¹…å·å£°æ˜æ¥æ¶ˆè´¹.png)

#### 6.5.2 åˆ›å»ºæŒä¹…å·

```yaml
apiVersion: v1
#å£°æ˜PV
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  #å£°æ˜PVå¤§å°
  capacity: 
    storage: 1Gi
  #è®¿é—®æ¨¡å¼
  #the volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple pods to access the volume when the pods are running on the same node.
  #the volume can be mounted as read-only by many nodes.
  accessModes:
    - ReadWriteOnce
    - ReadOnlyMany
  #PVå°†ä¸æ‰§è¡Œæ¸…ç†å’Œåˆ é™¤
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /tmp/mongodb
```

![å’Œé›†ç¾¤èŠ‚ç‚¹ä¸€æ ·ï¼ŒæŒä¹…å·ä¸å±äºä»»ä½•å‘½åç©ºé—´ï¼ŒåŒºåˆ«äºpodå’ŒæŒä¹…å·å£°æ˜](å’Œé›†ç¾¤èŠ‚ç‚¹ä¸€æ ·ï¼ŒæŒä¹…å·ä¸å±äºä»»ä½•å‘½åç©ºé—´ï¼ŒåŒºåˆ«äºpodå’ŒæŒä¹…å·å£°æ˜.png)

#### 6.5.3 é€šè¿‡åˆ›å»º PVC æ¥è·å–æŒä¹…å·

##### åˆ›å»ºæŒä¹…å·å£°æ˜

```yaml
apiVersion: v1
#å£°æ˜PVC
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc 
spec:
  #PVC çš„èµ„æºè¦æ±‚
  resources:
    requests:
      storage: 1Gi
  accessModes:
  - ReadWriteOnce
  #å’ŒåŠ¨æ€é…ç½®æœ‰å…³
  storageClassName: ""
```

```bash
#å¯ä»¥çœ‹åˆ° PVC å’Œ PV çš„çŠ¶æ€éƒ½å·²ç»å˜æˆ Bound äº†ã€‚

k get pvc
#NAME          STATUS   VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
#mongodb-pvc   Bound    mongodb-pv   1Gi        RWO,ROX                       105s

k get pv
#NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   REASON   AGE
#mongodb-pv   1Gi        RWO,ROX        Retain           Bound    default/mongodb-pvc                           9m45s
```

#### 6.5.4 åœ¨ pod ä¸­ä½¿ç”¨ PVC

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mongodb 
spec:
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
  #ä½¿ç”¨ PVC
  volumes:
  - name: mongodb-data
    persistentVolumeClaim:
      claimName: mongodb-pvc
```

##### è®¿é—® mongo

```bash
k exec -it mongodb -- mongo

use mystore
db.foo.insert({name:'foo'})
db.foo.find()
#{ "_id" : ObjectId("618205f0c383207666c6bdbb"), "name" : "foo" }
```

#### 6.5.5 äº†è§£ä½¿ç”¨ PV å’Œ PVC çš„å¥½å¤„

> ç›´æ¥ä½¿ç”¨çš„è¯ï¼Œpod å’ŒåŸºç¡€è®¾æ–½è€¦åˆäº†ï¼Œåœ¨ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°±å¿…é¡»ä½¿ç”¨GCEæŒä¹…ç£ç›˜ï¼›
>
> é€šè¿‡PVCå’ŒPVä½¿ç”¨çš„è¯ï¼Œæˆ‘ä»¬çš„ pod æ˜¯å¯å¤ç”¨çš„ï¼Œå½“éœ€è¦ä¿®æ”¹åŸºç¡€è®¾æ–½çš„æ—¶å€™ï¼Œåªéœ€è¦ä¿®æ”¹ PV å³å¯ã€‚

![ç›´æ¥ä½¿ç”¨ä¸é€šè¿‡PVCå’ŒPVä½¿ç”¨GCEæŒä¹…ç£ç›˜](ç›´æ¥ä½¿ç”¨ä¸é€šè¿‡PVCå’ŒPVä½¿ç”¨GCEæŒä¹…ç£ç›˜.png)

#### 6.5.6 å›æ”¶ PV

> é€šè¿‡ persistentVolumeReclaimPolicy æ¥æ§åˆ¶æŒä¹…å·çš„è¡Œä¸ºã€‚

```bash
#åˆ é™¤ pod
k delete pods mongodb
#åˆ é™¤ pvc
k delete pvc mongodb-pvc

#é‡æ–°åˆ›å»ºpvcå’Œpod
k create -f mongodb-pvc.yaml
k create -f mongodb-pod-pvc.yaml

#æˆ‘ä»¬å‘ç° pvc çš„çŠ¶æ€æ˜¯ pendingï¼Œå› ä¸º pv è¿˜æ²¡æœ‰æ¸…ç†ã€‚
k get pvc
#NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
#mongodb-pvc   Pending

#pv çš„çŠ¶æ€æ˜¯ released è€Œä¸æ˜¯ available
k get pv
#NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                 STORAGECLASS   REASON   AGE
#mongodb-pv   1Gi        RWO,ROX        Retain           Released   default/mongodb-pvc                           176m
```

### 6.6 pv çš„åŠ¨æ€å·é…ç½®

> - StorageClass
> - provisioner

#### 6.6.1 é€šè¿‡ StorageClass èµ„æºå®šä¹‰å¯ç”¨å­˜å‚¨ç±»å‹

> StorageClass æŒ‡å®šå½“ pvc è¯·æ±‚æ—¶åº”è¯¥ä½¿ç”¨å“ªä¸ªç¨‹åºæ¥æä¾› pvã€‚
>
> - sc.provisioner:Provisioner indicates the type of the provisioner.
> - sc.parameters:Parameters holds the parameters for the provisioner that should create volumes of this storage class.

```yaml
apiVersion: storage.k8s.io/v1
#æŒ‡å®šç±»å‹ä¸ºStorageClass
kind: StorageClass
metadata:
  name: fast
#ç”¨äºé…ç½® pv çš„å·æ’ä»¶
provisioner: k8s.io/minikube-hostpath
#ä¼ é€’ç»™parametersçš„å‚æ•°
parameters:
  type: pd-ssd
```

#### 6.6.2 è¯·æ±‚ pvc ä¸­çš„å­˜å‚¨ç±»

> åˆ›å»ºå£°æ˜æ—¶ï¼Œpv ç”± `fast` StorageClass èµ„æºä¸­å¼•ç”¨çš„ `provisioner` åˆ›å»ºã€‚

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc 
spec:
  #æŒ‡å®š StorageClass
  storageClassName: fast
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce
```

##### åˆ›å»ºä¸€ä¸ªæ²¡æœ‰æŒ‡å®šå­˜å‚¨ç±»åˆ«çš„ PVC

> ä¸è®¾ç½® storageClassName å°†ä½¿ç”¨æ²¡æœ‰æŒ‡å®šå­˜å‚¨ç±»åˆ«çš„ PVC

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc2 
spec:
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce
```

##### å¼ºåˆ¶å°† PVC ç»‘å®šåˆ°é¢„é…ç½®çš„å…¶ä¸­ä¸€ä¸ª PV

> å¦‚æœå¸Œæœ›PVCä½¿ç”¨é¢„å…ˆé…ç½®çš„PVï¼Œè¯·å°† storageClassName è®¾ç½®ä¸º ""ã€‚
>
> storageClassName å£°æ˜ä¸º "" å°†ç¦ç”¨åŠ¨æ€é…ç½®ã€‚

```bash
#å¯ä»¥çœ‹åˆ°
#è®¾ç½® storageClassName: "" çš„å°†ä¸ä½¿ç”¨ StorageClass è€Œæ˜¯åŒ¹é…ç¬¦åˆ PVC æ¡ä»¶çš„ PV
#ä¸è®¾ç½® storageClassName çš„å°†ä½¿ç”¨ StorageClass: standard
#è®¾ç½® storageClassName: fast çš„å°†ä½¿ç”¨ storageClassName: fast
k. get pv
#NAME                 CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                    STORAGECLASS   REASON   AGE
#mongodb-pv           1Gi        RWO,ROX        Retain           Bound      default/mongodb-pvc                              34s
#pvc-2ed965e5-5731-   100Mi      RWO            Delete           Bound      default/mongodb-pvc2     standard                30m
#pvc-6d13f5e1-ac80-   100Mi      RWO            Delete           Released   default/mongodb-pvc      fast                    4h7m
#pvc-6ef6feed-dde1-   100Mi      RWO            Delete           Bound      default/mongodb-pvc-dp   fast                    14m
```

![PVåŠ¨æ€é…ç½®](PVåŠ¨æ€é…ç½®.png)

## 7. ConfigMap å’Œ Secret: é…ç½®åº”ç”¨ç¨‹åº

### 7.1 é…ç½®å®¹å™¨åŒ–åº”ç”¨ç¨‹åº

> ä¸ºä»€ä¹ˆå¾ˆå¤šæ—¶å€™ docker é•œåƒé€šè¿‡ç¯å¢ƒå˜é‡ä¼ é€’é…ç½®å‚æ•°?
>
> 1. å°†é…ç½®æ–‡ä»¶æ‰“å…¥é•œåƒ,è¿™ç§ç±»ä¼¼äºç¡¬ç¼–ç ,æ¯æ¬¡å˜æ›´éœ€è¦é‡æ–° build é•œåƒï¼›
> 2. æŒ‚è½½å·,ä½†æ˜¯è¿™ç§æ–¹å¼éœ€è¦ä¿è¯é…ç½®ç¯å¢ƒåœ¨å®¹å™¨å¯åŠ¨ä¹‹å‰å†™å…¥åˆ°å·ä¸­ã€‚

### 7.2 å‘å®¹å™¨ä¼ é€’å‘½ä»¤è¡Œå‚æ•°

#### 7.2.1 åœ¨ docker ä¸­å®šä¹‰å‘½ä»¤ä¸å‚æ•°

##### äº†è§£ ENTRYPOINT å’Œ CMD

[Dockerfile: ENTRYPOINTå’ŒCMDçš„åŒºåˆ«](https://zhuanlan.zhihu.com/p/30555962)

> - å…±åŒç‚¹
>   - ENTRYPOINT å’Œ CMD éƒ½å¯ä»¥ç”¨æ¥åœ¨ docker é•œåƒæ„å»ºçš„è¿‡ç¨‹ä¸­æ‰§è¡ŒæŒ‡ä»¤
> - ä¸åŒç‚¹
>   - CMD æ›´å®¹æ˜“åœ¨ `docker run` çš„è¿‡ç¨‹ä¸­ä¿®æ”¹,è€Œ ENTRYPOINT éœ€è¦é€šè¿‡ `--entrypoint` è¦†ç›–

> 1. æ°¸è¿œä½¿ç”¨ `ENTRYPOINT ["/bin/ping","-c","3"]` è¿™ç§ exec è¡¨ç¤ºæ³•,å› ä¸º shell è¡¨ç¤ºæ³•çš„ä¸»è¿›ç¨‹(PID=1) æ˜¯ shell è¿›ç¨‹,è€Œæˆ‘ä»¬è¦å¯åŠ¨çš„ä¸»è¿›ç¨‹åè€Œæ˜¯é€šè¿‡ shell è¿›ç¨‹å¯åŠ¨çš„.
> 2. `ENTRYPOINT` å’Œ `CMD` å¯ä»¥æ··ç”¨,ä¸‹é¢çš„ä¾‹å­ä¸­ï¼ŒCMD å°†ä½œä¸º ENTRYPOINT çš„å‚æ•°ã€‚

```docker
FROM ubuntu:trusty

ENTRYPOINT ["/bin/ping","-c","3"]
CMD ["localhost"] 
```

> å®¹å™¨ä¸­è¿è¡Œçš„å®Œæ•´æŒ‡ä»¤ç”±ä¸¤éƒ¨åˆ†ç»„æˆ:å‘½ä»¤ä¸å‚æ•°
>
> - ENTRYPOINT å®šä¹‰å®¹å™¨å¯åŠ¨æ—¶è¢«è°ƒç”¨çš„å¯æ‰§è¡Œç¨‹åº;
> - CMD æŒ‡å®šä¼ é€’ç»™ ENTRYPOINT çš„å‚æ•°.

##### å¯é…ç½®åŒ– fortune é•œåƒä¸­çš„é—´éš”å‚æ•°

```bash
#!/bin/bash
trap "exit" SIGINT

INTERVAL=$1
echo Configured to generate new fortune every $INTERVAL seconds

mkdir -p /var/htdocs

while :
do
  echo $(date) Writing fortune to /var/htdocs/index.html
  /usr/games/fortune > /var/htdocs/index.html
  sleep $INTERVAL
done
```

```dockerfi
FROM ubuntu:latest

RUN sed -i s@/archive.ubuntu.com/@/mirrors.aliyun.com/@g /etc/apt/sources.list
RUN apt-get clean
RUN apt-get update
RUN apt-get -y install fortune

ADD fortuneloop.sh /bin/fortuneloop.sh

RUN chmod 755 /bin/fortuneloop.sh

ENTRYPOINT ["/bin/fortuneloop.sh"]
CMD ["10"]
```

```bash
docker build -t docker.io/luksa/fortune:args .

docker push docker.io/luksa/fortune:args

docker run -it docker.io/luksa/fortune:args
#Configured to generate new fortune every 10 seconds
#Mon Nov 8 07:14:25 UTC 2021 Writing fortune to /var/htdocs/index.html

docker run -it docker.io/luksa/fortune:args 15
#Configured to generate new fortune every 15 seconds
#Mon Nov 8 07:16:18 UTC 2021 Writing fortune to /var/htdocs/index.html
```

#### 7.2.2 åœ¨ kubernetes ä¸­è¦†ç›–å‘½ä»¤å’Œå‚æ•°

> ENTRYPOINT å’Œ CMD éƒ½å¯ä»¥è¢«è¦†ç›–ã€‚

| Docker     | Kubernetes |
| ---------- | ---------- |
| ENTRYPOINT | command    |
| CMD        | args       |

```yaml
kind: Pod
spec:
  containers:
    - image: som/image
      command: ["/bin/command"]
      args: ["arg1", "arg2", "arg3"]
```

##### ç”¨è‡ªå®šä¹‰é—´éš”å€¼è¿è¡Œ fortune pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: fortune2s
spec:
  containers:
  - image: luksa/fortune:args
    #é€šè¿‡ args ä¿®æ”¹å‚æ•°
    args: ["2"]
    name: html-generator
    #æŒ‚è½½å·
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  #å£°æ˜å·
  volumes:
  - name: html
    emptyDir: {}
```

### 7.3 ä¸ºå®¹å™¨è®¾ç½®ç¯å¢ƒå˜é‡

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: fortune-env
spec:
  containers:
  - image: luksa/fortune:env
    env:
    # ç¯å¢ƒå˜é‡
    - name: INTERVAL
      value: "30"
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    emptyDir: {}
```

#### 7.3.2 å¼•ç”¨å…¶ä»–ç¯å¢ƒå˜é‡

```yaml
env:
- name: FIRST_VAR
  value : "foo"
- name: SECOND_VAR
  value: "$(FIRST_VAR)bar"
```

### 7.4 ConfigMap è§£è€¦é…ç½®

#### 7.4.1 ConfigMap ä»‹ç»

> ConfigMap æ˜¯ kubernetes æä¾›çš„å•ç‹¬çš„èµ„æºå¯¹è±¡,é€šè¿‡ç¯å¢ƒå˜é‡æˆ–è€…å·æ–‡ä»¶ä¼ é€’ç»™å®¹å™¨.

![ConfigMap ä½¿ç”¨ç¯å¢ƒå˜é‡](ConfigMap ä½¿ç”¨ç¯å¢ƒå˜é‡.png)

#### 7.4.2 åˆ›å»º ConfigMap

```bash
# create ConfigMap
k create configmap fortune-config --from-literal=sleep-interval=25

k create configmap test-fortune-config --from-literal=foo=bar --from-literal=bar=foo --from-literal=on=two
```

![ConfigMap å®ä¾‹](ConfigMap å®ä¾‹.png)

```bash
k get configmaps fortune-config -o yaml
```

##### ä»å†…å®¹æ–‡ä»¶åˆ›å»º ConfigMap

```bash
# from config
k create configmap my-config --from-file=config-file.conf
```

#### 7.4.3 ä»å®¹å™¨ä¼ é€’ ConfigMap ä½œä¸ºç¯å¢ƒå˜é‡

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: fortune-env-from-configmap
spec:
  containers:
  - image: luksa/fortune:env
    env:
    #è®¾ç½®ç¯å¢ƒå˜é‡ INTERVAL
    - name: INTERVAL
      valueFrom: 
        #ç”¨ ConfigMap åˆå§‹åŒ–
        configMapKeyRef:
          #å¼•ç”¨çš„ ConfigMapåç§°
          name: fortune-config
          #ConfigMap ä¸‹å¯¹åº”çš„é”®çš„å€¼
          key: sleep-interval
# ...
```

#### 7.4.4 ä¸€æ¬¡æ€§ä¼ é€’ ConfigMap çš„æ‰€æœ‰æ¡ç›®ä½œä¸ºç¯å¢ƒå˜é‡

> å‡è®¾ ConfigMap åŒ…å«äº† FOO, BAR, FOO-BAR 

```yaml
spec:
  containers:
  - image: some-image
    # ä½¿ç”¨ envFrom å­—æ®µè€Œä¸æ˜¯ env å­—æ®µ
    envFrom:
    #æ‰€æœ‰ç¯å¢ƒå˜é‡å‡åŒ…å«å‰ç¼€ CONFIG_
    - prefix: CONFIG_
      configMapRef:
        #å¼•ç”¨åä¸º my-config-map çš„ ConfigMap
        name: my-config-map
```

> å¯¹äºä¸Šé¢çš„é…ç½®æ–‡ä»¶,æˆ‘ä»¬å°±å¾—åˆ°äº† CONFIG_FOO, CONFIG_BAR
>
> ä½†æ˜¯ä¸ä¼šæœ‰ **CONFIG_FOO-BAR**,å› ä¸ºè¿™ä¸æ˜¯ä¸€ä¸ªåˆæ³•çš„ç¯å¢ƒå˜é‡å,**åˆ›å»ºç¯å¢ƒå˜é‡æ—¶ä¼šå¿½ç•¥å¹¶ä¸”`ä¸ä¼š`å‘å‡ºäº‹ä»¶é€šçŸ¥**

#### 7.4.5 ä¼ é€’ ConfigMap æ¡ç›®ä½œä¸ºå‘½ä»¤è¡Œå‚æ•°

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: fortune-args-from-configmap
spec:
  containers:
  - image: luksa/fortune:args
    env:
    - name: INTERVAL
      valueFrom: 
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval
    #åœ¨å‚æ•°è®¾ç½®ä¸­å¼•ç”¨ç¯å¢ƒå˜é‡
    args: ["$(INTERVAL)"]
#...
```

#### 7.4.6 ä½¿ç”¨ configMap å·å°†æ¡ç›®æš´éœ²ä¸ºæ–‡ä»¶

> ç¯å¢ƒå˜é‡å’Œå‚æ•°é€‚ç”¨äºè½»é‡çš„åœºæ™¯.configMap ä¸­å¯ä»¥é€šè¿‡ä¸€ç§å« configMap çš„å·æ¥å®ç°é‡é‡çº§çš„é…ç½®åŠŸèƒ½.

##### my-nginx-config.conf

```config

server {
    listen              80;
    server_name         www.kubia-example.com;

    gzip on;
    gzip_types text/plain application/xml;

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }

}
```

##### sleep-interval

```txt
25
```

```bash
k create configmap fortune-config --from-file=../configmap-files

k get cm fortune-config -o yaml
#apiVersion: v1
#data:
#  my-nginx-config.conf: |
#    server {
#        listen              80;
#        server_name         www.kubia-example.com;
#
#        gzip on;
#        gzip_types text/plain application/xml;
#
#        location / {
#            root   /usr/share/nginx/html;
#            index  index.html index.htm;
#        }
#
#    }
#  sleep-interval: |
#    25
#kind: ConfigMap
#metadata:
#  creationTimestamp: "2021-11-08T09:59:37Z"
#  managedFields:
#  - apiVersion: v1
#    fieldsType: FieldsV1
#    fieldsV1:
#      f:data:
#        .: {}
#        f:my-nginx-config.conf: {}
#        f:sleep-interval: {}
#    manager: kubectl-create
#    operation: Update
#    time: "2021-11-08T09:59:37Z"
#  name: fortune-config
#  namespace: default
#  resourceVersion: "241091"
#  uid: c759fb6e-8464-4513-9e24-4b5b8b950691
```

##### åœ¨å·å†…ä½¿ç”¨ ConfigMap çš„æ¡ç›®

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: fortune-configmap-volume
spec:
  containers:
  - image: luksa/fortune:env
    #
    env:
    - name: INTERVAL
      valueFrom:
        #å¼•ç”¨ ConfigMap fortune-config çš„ sleep-interval
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    #æŒ‚åœ¨ configMap åˆ°å¯¹åº”ç›®å½•
    - name: config
      #nginx ä¼šè‡ªåŠ¨çš„è½½å…¥ /etc/nginx/conf.d ç›®å½•ä¸‹çš„æ‰€æœ‰ .conf é…ç½®æ–‡ä»¶
      mountPath: /etc/nginx/conf.d
      readOnly: true
    #æŒ‚åœ¨ configMap åˆ°å¯¹åº”ç›®å½•
    - name: config
      mountPath: /tmp/whole-fortune-config-volume
      readOnly: true
    ports:
      - containerPort: 80
        name: http
        protocol: TCP
  volumes:
  - name: html
    emptyDir: {}
  #ä½¿ç”¨ ConfigMap volume
  - name: config
    configMap:
      name: fortune-config
```

##### å·å†…æš´éœ²æŒ‡å®šçš„ ConfigMap æ¡ç›®

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: fortune-configmap-volume-with-items
spec:
  containers:
  - image: luksa/fortune:env
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    - name: config
      mountPath: /etc/nginx/conf.d/
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    emptyDir: {}
  - name: config
    configMap:
      #é€‰æ‹©åŒ…å«åœ¨å·ä¸­çš„æ¡ç›®
      name: fortune-config
      items:
      #è¯¥é”®å¯¹åº”çš„æ¡ç›®è¢«åŒ…å«
      - key: my-nginx-config.conf
        #æ¡ç›®çš„å€¼è¢«å­˜å‚¨åœ¨è¯¥æ–‡ä»¶ä¸­
        path: gzip.conf
```

```bash
k exec fortune-configmap-volume-with-items -c web-server -- ls /etc/nginx/conf.d
#gzip.conf
```

### 7.5 ä½¿ç”¨ Secret ç»™å®¹å™¨ä¼ é€’æ•æ„Ÿæ•°æ®

> Secret å’Œ ConfigMap ç±»ä¼¼,ä¹Ÿæ˜¯é”®å€¼å¯¹.

## 8. ä»åº”ç”¨è®¿é—®podå…ƒæ•°æ®ä»¥åŠå…¶ä»–èµ„æº

### 8.1 é€šè¿‡ Downward API ä¼ é€’å…ƒæ•°æ®

> Downward API å…è®¸æˆ‘ä»¬é€šè¿‡ç¯å¢ƒå˜é‡æˆ–è€…æ–‡ä»¶(åœ¨ downwardAPI å·ä¸­)ä¼ é€’ pod çš„å…ƒæ•°æ®

![DownwardAPI é€šè¿‡ç¯å¢ƒå˜é‡æˆ–æ–‡ä»¶å¯¹å¤–æš´éœ² pod å…ƒæ•°æ®](DownwardAPI é€šè¿‡ç¯å¢ƒå˜é‡æˆ–æ–‡ä»¶å¯¹å¤–æš´éœ² pod å…ƒæ•°æ®.png)

#### 8.1.2 é€šè¿‡ç¯å¢ƒå˜é‡æš´éœ²å…ƒæ•°æ®

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: downward
spec:
  containers:
  - name: main
    image: busybox
    command: ["sleep", "9999999"]
    resources:
      requests:
        cpu: 15m
        memory: 100Ki
      limits:
        cpu: 100m
        memory: 4Mi
    env:
    - name: POD_NAME
      #å¼•ç”¨ pod manifest ä¸­çš„å…ƒæ•°æ®åç§°å­—æ®µ,è€Œä¸æ˜¯è®¾å®šä¸€ä¸ªå…·ä½“çš„å€¼
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: SERVICE_ACCOUNT
      valueFrom:
        fieldRef:
          fieldPath: spec.serviceAccountName
    - name: CONTAINER_CPU_REQUEST_MILLICORES
      valueFrom:
        #å®¹å™¨è¯·æ±‚çš„CPUå’Œå†…å­˜ä½¿ç”¨é‡æ˜¯å¼•ç”¨ resourceFieldRef å­—æ®µè€Œä¸æ˜¯ fieldRef
        resourceFieldRef:
          resource: requests.cpu
          divisor: 1m
    #å¯¹äºèµ„æºç›¸å…³çš„å­—æ®µ,æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåŸºæ•°å•ä½,ä»è€Œç”Ÿæˆæ¯ä¸€éƒ¨åˆ†çš„å€¼
    - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES
      valueFrom:
        resourceFieldRef:
          resource: limits.memory
          divisor: 1Ki
```

#### 8.1.3 é€šè¿‡ downwardAPI å·æ¥ä¼ é€’å…ƒæ•°æ®

```yaml
apiVersion: v1
kind: Pod
metadata:
  #é€šè¿‡ downwardAPI å·æ¥æš´éœ²è¿™äº›æ ‡ç­¾å’Œæ³¨è§£
  name: downward
  labels:
    foo: bar
  annotations:
    key1: value1
    key2: |
      multi
      line
      value
spec:
  containers:
  - name: main
    image: busybox
    command: ["sleep", "9999999"]
    resources:
      requests:
        cpu: 15m
        memory: 100Ki
      limits:
        cpu: 100m
        memory: 4Mi
    #åœ¨ /etc/downward ä¸‹æŒ‚åœ¨ downward volume
    volumeMounts:
    - name: downward
      mountPath: /etc/downward
  volumes:
  #é€šè¿‡å°†å·åè®¾å®šä¸º downward æ¥å®šä¹‰ä¸€ä¸ª downwardAPI å·
  - name: downward
    downwardAPI:
      #å°† manifest æ–‡ä»¶ä¸­çš„ metadata.name å­—æ®µå†™å…¥ podName
      items:
      - path: "podName"
        fieldRef:
          fieldPath: metadata.name
      - path: "podNamespace"
        fieldRef:
          fieldPath: metadata.namespace
      #pod çš„æ ‡ç­¾å°†è¢«ä¿å­˜åˆ° /etc/downward/labels æ–‡ä»¶ä¸­,å› ä¸º volume è¢«æŒ‚åœ¨åœ¨ /etc/downward ä¸‹
      - path: "labels"
        fieldRef:
          fieldPath: metadata.labels
      - path: "annotations"
        fieldRef:
          fieldPath: metadata.annotations
      - path: "containerCpuRequestMilliCores"
        resourceFieldRef:
          containerName: main
          resource: requests.cpu
          divisor: 1m
      - path: "containerMemoryLimitBytes"
        resourceFieldRef:
          #å¿…é¡»æŒ‡å®šå®¹å™¨å
          containerName: main
          resource: limits.memory
          divisor: 1
```

```bash
k exec -it downward -- ls /etc/downward
#annotations                    labels
#containerCpuRequestMilliCores  podName
#containerMemoryLimitBytes      podNamespace

k exec -it downward -c main -- cat /etc/downward/annotations
#key1="value1"
#key2="multi\nline\nvalue\n"
#kubectl.kubernetes.io/default-container="main"
#...
```

##### ä¿®æ”¹ labels å’Œ annotations

> å¯ä»¥åœ¨podè¿è¡Œæ—¶ä¿®æ”¹æ ‡ç­¾å’Œæ³¨è§£,å½“æ ‡ç­¾å’Œæ³¨è§£è¢«ä¿®æ”¹ä¹‹åkubernetesä¼šæ›´æ–°å­˜æœ‰ç›¸å…³ä¿¡æ¯çš„æ–‡ä»¶.ä½†æ˜¯é€šè¿‡ç¯å¢ƒå˜é‡è¿è¡Œæ—¶æ˜¯ä¸ä¼šä¿®æ”¹çš„.
>
> è€Œæˆ‘ä»¬é€šè¿‡ downwardAPI å·åˆ™æ˜¯é€šè¿‡ `fieldRef` å¼•ç”¨çš„æ˜¯å¯ä»¥ç”Ÿæ•ˆçš„

##### åœ¨å·çš„å®šä¹‰ä¸­å¼•ç”¨å®¹å™¨çº§çš„å…ƒæ•°æ®

> å¼•ç”¨ `å®¹å™¨çº§` çš„å…ƒæ•°æ®,æ˜¯å› ä¸ºæˆ‘ä»¬å¯¹äºå·çš„å®šä¹‰æ˜¯ pod çº§çš„è€Œä¸æ˜¯å®¹å™¨çº§çš„.

```yaml
spec:
  volumes:
  - name: downward
    downwardAPI:
      items:
      - path: "containerCpuRequestMilliCores"
        resourceFieldRef:
          #å¿…é¡»æŒ‡å®šå®¹å™¨å
          containerName: main
          resource: requests.cpu
          divisor: 1m
```

### 8.2 ä¸ kubernetes API æœåŠ¡å™¨äº¤äº’

> é€šè¿‡ downwardAPI åªèƒ½æš´éœ²ä¸€ä¸ª pod è‡ªèº«çš„å…ƒæ•°æ®,ä¹Ÿåªèƒ½æš´éœ²ä¸€éƒ¨åˆ†å…ƒæ•°æ®.
>
> å¦‚æœæˆ‘ä»¬éœ€è¦è·å–æ‰€æœ‰çš„ pod çš„å…ƒæ•°æ®,å°±éœ€è¦ç›´æ¥ä¸APIæœåŠ¡å™¨è¿›è¡Œäº¤äº’.

![ä¸APIæœåŠ¡å™¨äº¤äº’](ä¸APIæœåŠ¡å™¨äº¤äº’.png)

#### 8.2.1 æ¢ç´¢ kubernetes REST API

```bash
# è·å–æœåŠ¡é›†ç¾¤ä¿¡æ¯
k cluster-info
#Kubernetes master is running at https://192.168.99.100:8443
#KubeDNS is running at https://192.168.99.100:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
#
#To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

##### é€šè¿‡ kubectl proxy è®¿é—® API æœåŠ¡å™¨

> kubectl proxy å¯åŠ¨ä¸€ä¸ªä»£ç†æœåŠ¡å™¨åšä»£ç†.

```bash
k proxy
#Starting to serve on 127.0.0.1:8001
```

##### é€šè¿‡ kubectl proxy ç ”ç©¶ kubernetes API

- `/api/v1` å¯¹åº” apiVersion

##### ç ”ç©¶æ‰¹é‡APIç»„çš„ REST endpoint

```bash
curl http://localhost:8001/apis/batch
```

```json
{
  "kind": "APIGroup",
  "apiVersion": "v1",
  "name": "batch",
  "versions": [
    {
      "groupVersion": "batch/v1",
      "version": "v1"
    },
    {
      "groupVersion": "batch/v1beta1",
      "version": "v1beta1"
    }
  ],
  "preferredVersion": {
    "groupVersion": "batch/v1",
    "version": "v1"
  }
}
```

> - versions æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º2çš„æ•°ç»„,å› ä¸ºå®ƒåŒ…å«äº† v1 å’Œ v1beta ä¸¤ä¸ªç‰ˆæœ¬
> - preferredVersion è¡¨ç¤ºå®¢æˆ·ç«¯åº”è¯¥ä½¿ç”¨ v1 ç‰ˆæœ¬

##### /apis/batch/v1

> - kind, apiVersion, groupVersion æ˜¯åœ¨ batch/v1 API ç»„ä¸­çš„APIèµ„æºæ¸…å•
> - resources åŒ…å«äº†è¿™ä¸ªç»„ä¸­æ‰€æœ‰çš„èµ„æºç±»å‹
> - resources[0].name å’Œ resources[1].name åˆ†åˆ«è¡¨ç¤ºäº† job èµ„æºä»¥åŠ job èµ„æºçš„çŠ¶æ€
> - resources[].verbs ç»™å‡ºäº†èµ„æºå¯¹åº”å¯ä»¥ä½¿ç”¨çš„æ“ä½œ.

```bash
curl http://localhost:8001/apis/batch/v1
```

```json
{
  "kind": "APIResourceList",
  "apiVersion": "v1",
  "groupVersion": "batch/v1",
  "resources": [
    {
      "name": "jobs",
      "singularName": "",
      "namespaced": true,
      "kind": "Job",
      "verbs": [
        "create",
        "delete",
        "deletecollection",
        "get",
        "list",
        "patch",
        "update",
        "watch"
      ],
      "categories": [
        "all"
      ],
      "storageVersionHash": "mudhfqk/qZY="
    },
    {
      "name": "jobs/status",
      "singularName": "",
      "namespaced": true,
      "kind": "Job",
      "verbs": [
        "get",
        "patch",
        "update"
      ]
    }
  ]
}
```

##### åˆ—ä¸¾é›†ç¾¤ä¸­æ‰€æœ‰çš„ job å®ä¾‹

> é€šè¿‡ kubernetes API å¾—åˆ°çš„ç»“æœå’Œ kubectl å¾—åˆ°çš„ç»“æœæ˜¯åŒ¹é…çš„.

```bash
curl http://localhost:8001/apis/batch/v1/jobs

k get jobs -A
```

##### é€šè¿‡åç§°restoreä¸€ä¸ªæŒ‡å®šçš„ job å®ä¾‹

> job å®ä¾‹
>
> - namespaces : ingress-nginx
> - name : ingress-nginx-admission-create

```bash
#Jobs
curl http://localhost:8001/apis/batch/v1/namespaces/ingress-nginx/jobs/ingress-nginx-admission-create
```

#### 8.2.2 ä» pod å†…éƒ¨ä¸ API æœåŠ¡å™¨äº¤äº’

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: curl
spec:
  containers:
  - name: main
    image: tutum/curl
    command: ["sleep", "9999999"]
```

```bash
# è¿›å…¥ curl
k exec -it curl -c main -- bash

# æŸ¥çœ‹ ca è¯ä¹¦
ls /var/run/secrets//kubernetes.io/serviceaccount/

# é€šè¿‡è¯ä¹¦è®¿é—®
curl --cacert /var/run/secrets//kubernetes.io/serviceaccount/ca.crt https://kubernetes

# export è¯ä¹¦ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥ç›´æ¥ curl
export CURL_CA_BUNDLE=var/run/secrets//kubernetes.io/serviceaccount/ca.crt

TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)

curl -H "Authorization: Bearer $TOKEN" https://kubernetes
```

##### å…³é—­åŸºäºè§’è‰²çš„æ§åˆ¶è®¿é—®ï¼ˆRBAC)

```bash
k create clusterrolebinding permissive-binding \
--clusterrole=cluster-admin \
--group=system:serviceaccounts
```

##### è·å–å½“å‰è¿è¡Œ pod æ‰€åœ¨çš„å‘½åç©ºé—´

```bash
NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)

# è·å–å‘½åç©ºé—´ä¸‹çš„æ‰€æœ‰ pods
curl -H "Authorization: Bearer $TOKEN" https://kubernetes/api/v1/namespaces/$NS/pods
```

#### 8.2.3 é€šè¿‡ ambassador å®¹å™¨ç®€åŒ–ä¸ API æœåŠ¡å™¨çš„äº¤äº’

##### ambassador å®¹å™¨æ¨¡å¼ä»‹ç»

![ä½¿ç”¨ ambassadorè¿æ¥APIæœåŠ¡å™¨](ä½¿ç”¨ ambassadorè¿æ¥APIæœåŠ¡å™¨.png)

##### è¿è¡Œå¸¦æœ‰é™„åŠ  ambassador å®¹å™¨çš„ CURL pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: curl-with-ambassador
spec:
  containers:
  - name: main
    image: tutum/curl
    command: ["sleep", "9999999"]
  # é¢å¤–è¿è¡Œä¸€ä¸ª ambassador å®¹å™¨
  - name: ambassador
    image: luksa/kubectl-proxy:1.6.2
```

```bash
# 
k exec -it curl-with-ambassador -c main -- bash

# ç›´æ¥è®¿é—® ambassador å®¹å™¨å†…çš„ kubectl proxy
curl localhost:8001
```

#### 8.2.4 ä½¿ç”¨å®¢æˆ·ç«¯åº“ä¸APIæœåŠ¡å™¨äº¤äº’

```go
package main

import (
	"context"
	"testing"
	"time"

	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/wait"
	"k8s.io/apimachinery/pkg/watch"
	"k8s.io/client-go/informers"
	"k8s.io/client-go/kubernetes/fake"
	clienttesting "k8s.io/client-go/testing"
	"k8s.io/client-go/tools/cache"
)

// TestFakeClient demonstrates how to use a fake client with SharedInformerFactory in tests.
func TestFakeClient(t *testing.T) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	watcherStarted := make(chan struct{})
	// Create the fake client.
	client := fake.NewSimpleClientset()
	// A catch-all watch reactor that allows us to inject the watcherStarted channel.
	client.PrependWatchReactor("*", func(action clienttesting.Action) (handled bool, ret watch.Interface, err error) {
		gvr := action.GetResource()
		ns := action.GetNamespace()
		watch, err := client.Tracker().Watch(gvr, ns)
		if err != nil {
			return false, nil, err
		}
		close(watcherStarted)
		return true, watch, nil
	})

	// We will create an informer that writes added pods to a channel.
	pods := make(chan *v1.Pod, 1)
	informers := informers.NewSharedInformerFactory(client, 0)
	podInformer := informers.Core().V1().Pods().Informer()
	podInformer.AddEventHandler(&cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			pod := obj.(*v1.Pod)
			t.Logf("pod added: %s/%s", pod.Namespace, pod.Name)
			pods <- pod
		},
	})

	// Make sure informers are running.
	informers.Start(ctx.Done())

	// This is not required in tests, but it serves as a proof-of-concept by
	// ensuring that the informer goroutine have warmed up and called List before
	// we send any events to it.
	cache.WaitForCacheSync(ctx.Done(), podInformer.HasSynced)

	// The fake client doesn't support resource version. Any writes to the client
	// after the informer's initial LIST and before the informer establishing the
	// watcher will be missed by the informer. Therefore we wait until the watcher
	// starts.
	// Note that the fake client isn't designed to work with informer. It
	// doesn't support resource version. It's encouraged to use a real client
	// in an integration/E2E test if you need to test complex behavior with
	// informer/controllers.
	<-watcherStarted
	// Inject an event into the fake client.
	p := &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "my-pod"}}
	_, err := client.CoreV1().Pods("test-ns").Create(context.TODO(), p, metav1.CreateOptions{})
	if err != nil {
		t.Fatalf("error injecting pod add: %v", err)
	}

	select {
	case pod := <-pods:
		t.Logf("Got pod from channel: %s/%s", pod.Namespace, pod.Name)
	case <-time.After(wait.ForeverTestTimeout):
		t.Error("Informer did not get the added pod")
	}
}
```

## 9. Deploymentï¼š å£°æ˜å¼çš„å‡çº§åº”ç”¨

> Deployment æ˜¯ä¸€ç§åŸºäº ReplicaSet çš„èµ„æºã€‚

### 9.1 æ›´æ–°è¿è¡Œåœ¨podå†…çš„åº”ç”¨ç¨‹åº

pod åœ¨å‘å¸ƒæ–°ç‰ˆæœ¬æ—¶æœ‰ä¸¤ç§æ–¹æ³•ï¼š

- åˆ é™¤ç°æœ‰podï¼Œåˆ›æ–°æ–° pod
- åˆ›å»ºæ–° podï¼Œç„¶åä¸€æ¬¡æ€§åˆ é™¤è€çš„ podï¼Œæˆ–è€…æŒ‰ç…§é¡ºåºåˆ›å»ºæ–°çš„ podï¼Œç„¶åä¸€æ¬¡åˆ›å»ºè€çš„ pod

ä¸¤ç§æ–¹æ³•éƒ½æœ‰ä¸€å®šçš„é—®é¢˜ï¼š

1. ç¬¬ä¸€ç§æ–¹æ³•ä¼šå¯¼è‡´æœåŠ¡æœ‰ä¸€æ®µæ—¶é—´ä¸å¯ç”¨ï¼›
2. ç¬¬äºŒç§æ–¹æ³•ä¼šå¯¼è‡´æœåŠ¡ç‰ˆæœ¬ä¸ä¸€è‡´ï¼Œå¦‚æœå­˜åœ¨çŠ¶æ€ï¼ˆæ¯”å¦‚å†™å…¥æ•°æ®åº“ï¼‰å¯èƒ½ä¼šå¯¼è‡´æ•°æ®å¼‚å¸¸ï¼›

#### 9.1.1 åˆ é™¤æ—§ç‰ˆæœ¬ podï¼Œä½¿ç”¨æ–°ç‰ˆæœ¬ pod æ›¿æ¢

> å¦‚æœå¯ä»¥æ¥å—çŸ­æš‚æœåŠ¡ä¸å¯ç”¨ï¼Œé‚£å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ–¹æ¡ˆ

#### 9.1.2 å…ˆåˆ›å»ºæ–°podï¼Œå†åˆ é™¤æ—§ç‰ˆæœ¬pod

> ä¸‹é¢å°±æ˜¯æ‰€è°“çš„ **è“ç»¿éƒ¨ç½²**

![å°†serviceæµé‡ä»æ—§çš„podè¿ç§»åˆ°æ–°çš„pod](å°†serviceæµé‡ä»æ—§çš„podè¿ç§»åˆ°æ–°çš„pod.png)

> æ»šåŠ¨å‡çº§

![æ»šåŠ¨å‡çº§](æ»šåŠ¨å‡çº§.png)

### 9.2 ä½¿ç”¨ rc å®ç°è‡ªåŠ¨çš„æ»šåŠ¨å‡çº§

> è¿™æ˜¯ä¸€ç§ç›¸å¯¹è¿‡æ—¶çš„å‡çº§æ–¹å¼ï¼Œä¸è¿‡ä¸ç”¨å¼•å…¥æ–°çš„æ¦‚å¿µã€‚

#### 9.2.1 è¿è¡Œç¬¬ä¸€ä¸ªç‰ˆæœ¬çš„åº”ç”¨

```js
// app.js v1
const http = require('http');
const os = require('os');

console.log("Kubia server starting...");

var handler = function(request, response) {
  console.log("Received request from " + request.connection.remoteAddress);
  response.writeHead(200);
  response.end("This is v1 running in pod " + os.hostname() + "\n");
};

var www = http.createServer(handler);
www.listen(8080);

```

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia-v1
spec:
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1
        name: nodejs
---
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  type: LoadBalancer
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
```

#### 9.2.2 ä½¿ç”¨ kubectl æ¥æ‰§è¡Œæ»šåŠ¨å‡çº§

##### ä½¿ç”¨ç›¸åŒçš„ tag æ¨é€æ›´æ–°è¿‡åçš„é•œåƒ

> å¦‚æœä½¿ç”¨äº†ä¸€ä¸ªé latest çš„é•œåƒ tagï¼Œä¾‹å¦‚ v1ã€‚é‚£ä¹ˆ kubernetes åœ¨å…ˆæ‹‰å–æœ¬åœ°ç¼“å­˜ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯èƒ½ä¼šå¯¼è‡´å˜æ›´æ— æ³•ç”Ÿæ•ˆã€‚
>
> å¯ä»¥è®¾ç½®å®¹å™¨çš„ `imagePullPolicy` è®¾ç½®ä¸º always

```yaml
# æ»šåŠ¨å‡çº§ï¼Œä¸è¿‡ rolling-update lastest å·²ç»åºŸå¼ƒ
k rollout kubia-v1 kubia-v2 --image=luksa/kubia:v2
```

![å¼€å§‹æ»šåŠ¨å‡çº§åçš„çŠ¶æ€](å¼€å§‹æ»šåŠ¨å‡çº§åçš„çŠ¶æ€.png)

##### äº†è§£æ»šåŠ¨å‡çº§å‰ kubectl æ‰€æ‰§è¡Œçš„æ“ä½œ

> åœ¨æ»šåŠ¨å‡çº§å‰ï¼Œkubernetes ä¼šä¿®æ”¹è¿™äº›å±æ€§ï¼š
>
> 1. ä¸º v2 rc çš„ selector å¢åŠ  `deployment=xxx`ï¼Œè¿™æ˜¯ä¸ºäº†é˜²æ­¢ v2 rc åŒ¹é…åˆ°è€çš„ v1 pod
> 2. ä¸º v1 rc çš„ selector å¢åŠ  `deployment=yyy`ï¼Œè¿™æ˜¯ä¸ºäº†é˜²æ­¢ v1 rc åŒ¹é…åˆ° v2 pod
> 3. ä¸º v1 pod å¢åŠ  `deployment=yyy`ï¼Œé˜²æ­¢ v1 rc åŒ¹é…ä¸åˆ° pod

![æ»šåŠ¨å‡çº§åæ–°æ—§rcä»¥åŠpodçš„è¯¦ç»†çŠ¶æ€](æ»šåŠ¨å‡çº§åæ–°æ—§rcä»¥åŠpodçš„è¯¦ç»†çŠ¶æ€.png)

##### é€šè¿‡ä¼¸ç¼©ä¸¤ä¸ª rc å®Œæˆæ»šåŠ¨å‡çº§

> ä¸æ–­çš„é€šè¿‡å‡å° v1 pod çš„å‰¯æœ¬æ•°ï¼Œç„¶åå¢å¤§ v2 pod çš„å‰¯æœ¬æ•°ã€‚ç›´åˆ° v1 pod å‰¯æœ¬æ•°å‡å°åˆ° 0.

#### 9.2.3 ä¸ºä»€ä¹ˆ rolling-update è¿‡æ—¶äº†

> 1. è¿™ä¸ªè¿‡ç¨‹ä¼šç›´æ¥ä¿®æ”¹åˆ›å»ºçš„å¯¹è±¡ï¼ˆä¼šä¿®æ”¹ pod å’Œ rc çš„æ ‡ç­¾ï¼‰
> 2. **kubectl åªæ˜¯æ‰§è¡Œæ»šåŠ¨å‡çº§çš„å®¢æˆ·ç«¯**ï¼Œä¾‹å¦‚ï¼šåœ¨ä¸Šé¢çš„è¿‡ç¨‹ä¸­ï¼Œkubectl ä¼šç»™æœåŠ¡ç«¯å‘é€ä¸‰æ¬¡ä¿®æ”¹ v1 pod å‰¯æœ¬æ•°çš„å‘½ä»¤ï¼Œè€Œä¸æ˜¯ kubernetes master æ‰§è¡Œçš„ã€‚

### 9.3 ä½¿ç”¨ Deployment å£°æ˜å¼çš„å‡çº§åº”ç”¨

![Deployment](./Deployment.png)

> ä¸ºä»€ä¹ˆè¦åœ¨ rs æˆ–è€… rc ä¸Šå¼•å…¥ä¸€ä¸ªå¯¹è±¡ï¼Ÿ
>
> å› ä¸ºåœ¨ <9.2> çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è¦å‡çº§ä¸€ä¸ªåº”ç”¨çš„æ—¶å€™ï¼Œéœ€è¦å¼•å…¥ä¸€ä¸ªé¢å¤–çš„ rcï¼Œå¹¶åè°ƒä¸¤ä¸ª rcã€‚
>
> Deployment å°±æ˜¯ä¸ºäº†è´Ÿè´£è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

#### 9.3.1 åˆ›å»ºä¸€ä¸ª Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1
        name: nodejs
  selector:
    matchLabels:
      app: kubia
```

##### åˆ›å»º Deployment èµ„æº

```bash
# --record ä¼šè®°å½•å†å²ç‰ˆæœ¬å·
k create -f kubia-deployment-v1.yaml --record
```

##### å±•ç¤º Deployment æ»šåŠ¨è¿‡ç¨‹ä¸­çš„çŠ¶æ€

```bash
# çŠ¶æ€
k rollout status deployment kubia
```

##### äº†è§£ Deployment å¦‚ä½•åˆ›å»º rs ä»¥åŠ pod

> rs åŒ…å«äº† pod æ¨¡æ¿çš„å“ˆå¸Œå€¼ã€‚

```bash
k get deploy -o wide --show-labels
#kubia   3/3     3            3           138m   nodejs       luksa/kubia:v1   app=kubia   <none>


k get rs -o wide --show-labels
#kubia-74967b5695   3         3         3       140m   nodejs       luksa/kubia:v1   app=kubia,pod-template-hash=74967b5695   app=kubia,pod-template-hash=74967b5695

k get pods -o wide --show-labels
#kubia-74967b5695   3         3         3       139m   nodejs       luksa/kubia:v1   app=kubia,pod-template-hash=74967b5695   app=kubia,pod-template-hash=74967b5695
```

##### é€šè¿‡ service è®¿é—® pod

> deploy åˆ›å»ºäº† rsï¼Œrs åˆ›å»ºäº† podã€‚
>
> deploy é€šè¿‡ `app=kubia` åŒ¹é… rsï¼Œrs é€šè¿‡ `app=kubia,pod-template-hash=74967b5695` æ¥åŒ¹é… pods

#### 9.3.2 å‡çº§ Deployment

##### ä¸åŒçš„ Deployment å‡çº§ç­–ç•¥

> - RollingUpdateï¼ˆé»˜è®¤ï¼‰
> - Recreate

##### æ¼”ç¤ºå¦‚ä½•å‡æ…¢æ»šåŠ¨å‡çº§é€Ÿåº¦

```bash
k patch deploy kubia -p '{"spec": {"minReadySeconds": 10}}'
```

##### è§¦å‘æ»šåŠ¨å‡çº§

```bash
#è§¦å‘æ»šåŠ¨å‡çº§
#nodejs æ˜¯ deploy.spec.template.spec.containers.name
k set image deployment kubia nodejs=luksa/kubia:v2
```

![ä¸ºdeployå†…çš„podæ¨¡æ¿æŒ‡å®šæ–°çš„é•œåƒ](ä¸ºdeployå†…çš„podæ¨¡æ¿æŒ‡å®šæ–°çš„é•œåƒ.png)

##### Deployment çš„ä¼˜ç‚¹

> æ³¨æ„ï¼Œdeployment å¼•ç”¨ ConfigMap æˆ–è€… Secretï¼Œæ›´æ”¹ ConfigMap æˆ– Secret ä¸ä¼šè§¦å‘å‡çº§æ“ä½œã€‚
>
> å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„ ConfigMapï¼Œå¹¶ä¿®æ”¹ pod æ¨¡æ¿å¼•ç”¨æ–°çš„ ConfigMap æ¥å®ç°æ›´æ–°ã€‚

![æ»šåŠ¨å‡çº§å¼€å§‹å’Œç»“æŸæ—¶çš„deploymentçŠ¶æ€](æ»šåŠ¨å‡çº§å¼€å§‹å’Œç»“æŸæ—¶çš„deploymentçŠ¶æ€.png)

#### 9.3.3 å›æ»šdeployment

```bash
# éƒ¨ç½² v3 ç‰ˆæœ¬
k set image deployment kubia nodejs=luksa/kubia:v3

# æŸ¥çœ‹å›æ»šæ—¥å¿—
k rollout status deployment kubia

# å›æ»šå‡çº§
k rollout undo deployment kubia

# æ˜¾ç¤ºæ»šåŠ¨å‡çº§å†å²
k rollout history deployment kubia

# åˆ‡æ¢åˆ°æŒ‡å®šç‰ˆæœ¬
k rollout undo deployment kubia --to-revision=1
```

#### 9.3.5 æš‚åœæ»šåŠ¨å‡çº§

```bash
# æ»šåŠ¨å‡çº§åˆ° v4
k set image deployment kubia nodejs=luksa/kubia:v4

# æš‚åœæ»šåŠ¨å‡çº§
k rollout pause deployment kubia

# æ¢å¤æ»šåŠ¨å‡çº§
k rollout resume deployment kubia
```

#### 9.3.6 ç»„ç»‡å‡ºé”™ç‰ˆæœ¬çš„æ»šåŠ¨å‡çº§

##### äº†è§£ minReadySeconds çš„ç”¨å¤„

> minReadySeconds æŒ‡å®š pod è‡³å°‘è¿è¡ŒæˆåŠŸå¤šä¹…ä¹‹åï¼Œæ‰èƒ½å°†å…¶è§†ä¸ºå¯ç”¨ã€‚åœ¨è¿™ä¹‹å‰ï¼Œæ»šåŠ¨å‡çº§çš„è¿‡ç¨‹ä¸ä¼šç»§ç»­ã€‚ 

##### é…ç½®å°±ç»ªæŒ‡é’ˆé˜»æ­¢å…¨éƒ¨ v3 ç‰ˆæœ¬çš„æ»šåŠ¨éƒ¨ç½²

> pod åœ¨çŠ¶æ€ **ä¿æŒå°±ç»ªï¼Œå¹¶æŒç»­10s** æ‰ä¼šå¼€å§‹éƒ¨ç½²ä¸‹ä¸€ä¸ª podã€‚

```yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  # è®¾ç½® pod è‡³å°‘éœ€è¦è¿è¡ŒæˆåŠŸ 10s
  minReadySeconds: 10
  strategy:
    rollingUpdate:
      maxSurge: 1
      # è®¾ç½®é›†ç¾¤å†…æœ€å¤§ä¸å¯ä»¥ pod æ•°
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v3
        name: nodejs
        readinessProbe:
          # å°±ç»ªæŒ‡é’ˆæ¯ç§’æ‰§è¡Œä¸€æ¬¡
          periodSeconds: 1
          # http æŒ‡é’ˆ
          httpGet:
            path: /
            port: 8080
```

## 10. StatefulSetï¼šéƒ¨ç½²æœ‰çŠ¶æ€çš„å¤šå‰¯æœ¬åº”ç”¨

### 10.1 å¤åˆ¶æœ‰çŠ¶æ€ pod

> rs æ˜¯åœ¨ `template` é‡Œå…³è” pvc çš„ï¼Œåˆä¼šä¾æ® `template` å®ä¾‹åŒ–å¤šä¸ª podï¼Œåˆ™ä¸èƒ½å¯¹æ¯ä¸ªå‰¯æœ¬éƒ½æŒ‡å®šç‹¬ç«‹çš„ pvcã€‚

![rsç»‘å®špvå’Œpvc](rsç»‘å®špvå’Œpvc.png)

#### 10.1.2 æ¯ä¸ª pod éƒ½æä¾›ç¨³å®šçš„æ ‡è¯†

> å½“å¯åŠ¨çš„å®ä¾‹æ‹¥æœ‰å…¨æ–°çš„ç½‘ç»œæ ‡è¯†ï¼Œä½†æ˜¯è¿˜ä½¿ç”¨æ—§å®ä¾‹çš„æ•°æ®å¯èƒ½ä¼šå¼•èµ·é—®é¢˜ã€‚
>
> æ‰€ä»¥æˆ‘ä»¬å¸Œæœ› pod æ‹¥æœ‰ç¨³å®šçš„æ ‡è¯†ï¼ˆä¾‹å¦‚é‡å¯ä¹‹åIPä¸ä¼šå˜æ›´ï¼‰ã€‚

### 10.2 StatefulSet

> ä½¿ç”¨ StatefulSet ä»£æ›¿ ReplicaSet

#### 10.2.2 æä¾›ç¨³å®šçš„ç½‘ç»œæ ‡è¯†

![StatefulSetä¸»æœºå](StatefulSetä¸»æœºå.png)

##### æ§åˆ¶æœåŠ¡ä»‹ç»

> å¯¹äºæœ‰çŠ¶æ€çš„æœåŠ¡æ¥è¯´ï¼Œæˆ‘ä»¬é€šå¸¸æ˜¯å¸Œæœ›æ“ä½œçš„æ˜¯ Set ä¸­ **ç‰¹å®šçš„ä¸€ä¸ª**

##### æ›¿æ¢æ¶ˆå¤±çš„ pod

> é€šå¸¸æ¥è¯´ï¼Œå‡è®¾ä¸€ä¸ª `pod A-0` æ‰€åœ¨çš„ node å‘ç”Ÿæ•…éšœï¼Œé‚£ä¹ˆ `pod A-0` ä¼šåœ¨ä¸€ä¸ªå…¨æ–°çš„èŠ‚ç‚¹ä¸Šå¯åŠ¨ï¼Œå¹¶ä¸”ä»–çš„åå­—è¿˜æ˜¯ `pod A-0`

##### æ‰©ç¼©å®¹ StatefulSet

> - æ‰©å®¹æ—¶å°†ä½¿ç”¨ä¸‹ä¸€ä¸ªæ²¡æœ‰è¢«ä½¿ç”¨çš„ç´¢å¼•
> - ç¼©å®¹æ—¶å°†åˆ é™¤å½“å‰æœ€å¤§çš„ç´¢å¼•

> **StatefulSet åœ¨ç¼©å®¹æ—¶åªä¼šæ“ä½œä¸€ä¸ª pod å®ä¾‹ï¼Œå¹¶ä¸”åœ¨é›†ç¾¤æœ‰ä¸å¥åº·èŠ‚ç‚¹çš„æ—¶å€™ä¹Ÿä¸å…è®¸æ‰§è¡Œç¼©å®¹æ“ä½œã€‚**

#### ä¸ºæ¯ä¸ªæœ‰çŠ¶æ€å®ä¾‹æä¾›ç¨³å®šçš„ä¸“å±å­˜å‚¨

##### åœ¨ pod æ¨¡æ¿ä¸­æ·»åŠ å·å£°æ˜æ¨¡æ¿

> pvc ä¼šåœ¨**åˆ›å»ºpodå‰**åˆ›å»ºå‡ºæ¥å¹¶ç»‘å®šåˆ°ä¸€ä¸ªpodå®ä¾‹ä¸Šã€‚

![StatefulSetåˆ›å»ºpodå’Œpvc](StatefulSetåˆ›å»ºpodå’Œpvc.png)

##### æŒä¹…å·çš„åˆ›å»ºå’Œåˆ é™¤

> - æ‰©å®¹ StatefulSet æ—¶ï¼Œä¼šåˆ›å»ºä¸€ä¸ª pod å’Œä¸€ä¸ªæˆ–å¤šä¸ª pvc
> - ç¼©å®¹æ—¶ï¼Œä¼šåˆ é™¤ pod å¹¶ä¿ç•™ pvcï¼Œå› ä¸ºåˆ é™¤ pvc ä¼šå¯¼è‡´ pv è¢«åˆ é™¤ï¼Œä¸Šé¢çš„æ•°æ®ä¼šä¸¢å¤±ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨çš„åˆ é™¤ pvcã€‚

##### é‡æ–°æŒ‚è½½æŒä¹…å·å£°æ˜åˆ°ç›¸åŒpodçš„æ–°å®ä¾‹ä¸Š

![é‡æ–°æŒ‚è½½pvcåˆ°ç›¸åŒçš„podå®ä¾‹ä¸Š](é‡æ–°æŒ‚è½½pvcåˆ°ç›¸åŒçš„podå®ä¾‹ä¸Š.png)

#### 10.2.4 StatefulSet çš„ä¿éšœ

##### ç¨³å®šæ ‡è¯†å’Œç‹¬ç«‹å­˜å‚¨çš„å½±å“

> - å¯¹äº StatefulSetï¼Œkubernetes æ€»æ˜¯ä¿è¯å®ƒä¼šè¢«ä¸€ä¸ªå®Œå…¨ä¸€è‡´çš„ pod æ›¿æ¢ï¼ˆç›¸åŒçš„åç§°ï¼Œä¸»æœºåå’Œå­˜å‚¨ç­‰ï¼‰ï¼›

### 10.3 ä½¿ç”¨ Stateful

#### 10.3.1 åˆ›å»ºåº”ç”¨å’Œå®¹å™¨é•œåƒ

```javascript
const http = require('http');
const os = require('os');
const fs = require('fs');

const dataFile = "/var/data/kubia.txt";

function fileExists(file) {
  try {
    fs.statSync(file);
    return true;
  } catch (e) {
    return false;
  }
}

var handler = function(request, response) {
  // åœ¨postè¯·æ±‚ä¸­æŠŠbodyå­˜å‚¨åˆ°ä¸€ä¸ªæ–‡ä»¶
  if (request.method == 'POST') {
    var file = fs.createWriteStream(dataFile);
    file.on('open', function (fd) {
      request.pipe(file);
      console.log("New data has been received and stored.");
      response.writeHead(200);
      response.end("Data stored on pod " + os.hostname() + "\n");
    });
  } else {
	// åœ¨ get è¯·æ±‚ä¸­è¿”å›ä¸»æœºåå’Œæ•°æ®æ–‡ä»¶çš„å†…å®¹
    var data = fileExists(dataFile) ? fs.readFileSync(dataFile, 'utf8') : "No data posted yet";
    response.writeHead(200);
    response.write("You've hit " + os.hostname() + "\n");
    response.end("Data stored on this pod: " + data + "\n");
  }
};

var www = http.createServer(handler);
www.listen(8080);
```

#### 10.3.2 é€šè¿‡ StatefulSet æ¥éƒ¨ç½²åº”ç”¨

##### ä¸ºä»€ä¹ˆéœ€è¦ headless server

> ä¸æ™®é€šçš„ pod ä¸ä¸€æ ·çš„æ˜¯ï¼Œæœ‰çŠ¶æ€çš„ pod æœ‰æ—¶å€™éœ€è¦é€šè¿‡ä¸»æœºåæ¥å®šä½ï¼Œè€Œæ— çŠ¶æ€çš„ pod åˆ™ä¸éœ€è¦ã€‚å¯¹äºæœ‰çŠ¶æ€çš„ pod æ¥è¯´ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›æ“ä½œçš„æ˜¯å…¶ä¸­ç‰¹å®šçš„ä¸€ä¸ªã€‚
>
> åŸºäºä»¥ä¸ŠåŸå› ï¼Œä¸€ä¸ª StatefulSet é€šå¸¸è¦æ±‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç”¨æ¥è®°å½•æ¯ä¸ª pod ç½‘ç»œæ ‡è®°çš„ headless serviceã€‚é€šè¿‡è¿™ä¸ª service æ¯ä¸ª pod å°†æ‹¥æœ‰ç‹¬ç«‹çš„ DNS è®°å½•ã€‚
>
> æ¯”å¦‚ï¼Œä¸€ä¸ªåœ¨ default å‘½åç©ºé—´ï¼Œåä¸º foo çš„æ§åˆ¶æœåŠ¡ï¼Œå®ƒçš„ä¸€ä¸ª pod åç§°ä¸º A-0ï¼Œé‚£ä¹ˆå¯ä»¥é€šè¿‡ **a-0.foo.default.svc.cluster.local** æ¥è®¿é—®ï¼Œè¿™åœ¨ rs ä¸­æ˜¯è¡Œä¸é€šçš„ã€‚

##### åˆ›å»º pv

```yaml
#æè¿°éœ€è¦åˆ›å»ºä¸‰ä¸ª pv
kind: List
apiVersion: v1
items:
- apiVersion: v1
  #æŒ‡å®šéœ€è¦åˆ›å»ºçš„æ˜¯ pv
  kind: PersistentVolume
  metadata:
    name: pv-a
  spec:
    capacity:
      storage: 1Mi
    accessModes:
      - ReadWriteOnce
    #å½“å·è¢«å£°æ˜é‡Šæ”¾åï¼Œç©ºé—´ä¼šè¢«å›æ”¶å†åˆ©ç”¨
    #æ‰€ä»¥ï¼Œæˆ‘ä»¬åœ¨å’Œ StatefulSet åˆå¹¶ä½¿ç”¨çš„æ—¶å€™ï¼Œss ä¸ä¼šè‡ªåŠ¨çš„æ¸…ç† pvc ä»¥é¿å…è¯¥å·è¢«å›æ”¶
    persistentVolumeReclaimPolicy: Recycle
    hostPath:
      path: /tmp/pv-a
- apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv-b
  spec:
    capacity:
      storage: 1Mi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    hostPath:
      path: /tmp/pv-b
- apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv-c
  spec:
    capacity:
      storage: 1Mi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    hostPath:
      path: /tmp/pv-c
```

##### åˆ›å»ºæ§åˆ¶ service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  #æŒ‡å®š headless æ¨¡å¼
  clusterIP: None
  selector:
    app: kubia
  ports:
  - name: http
    #è¿™é‡Œæ³¨æ„ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸éœ€è¦é…ç½® targetPort: 8080ï¼Œè€Œæ˜¯åœ¨è®¿é—®æ—¶æ‰‹åŠ¨çš„åŠ ä¸Š 8080 ç«¯å£
    #å› ä¸ºè¿™æ˜¯ headless æ¨¡å¼ï¼Œæˆ‘ä»¬æ˜¯ç›´è¿ pod çš„ã€‚
    port: 80
```

##### åˆ›å»º StatefulSet æ¸…å•

```yaml
apiVersion: apps/v1
#æŒ‡å®šä½¿ç”¨ ss
kind: StatefulSet
metadata:
  name: kubia
spec:
  serviceName: kubia
  replicas: 2
  #StatefulSet åˆ›å»ºçš„ pod éƒ½å¸¦æœ‰ app=kubia æ ‡ç­¾
  selector:
    matchLabels:
      # has to match .spec.template.metadata.labels
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia-pet
        ports:
        - name: http
          containerPort: 8080
        volumeMounts:
        - name: data
          #æŒ‡å®š pv è¢«ç»‘å®šåˆ° /var/data
          mountPath: /var/data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      resources:
        requests:
          storage: 1Mi
      accessModes:
      - ReadWriteOnce
```

##### å…³è” StatefulSet

> StatefulSet ä¼šç­‰å¾…ç¬¬ä¸€ä¸ª pod å¯åŠ¨å¹¶å°±ç»ªä¹‹åå†å¯åŠ¨ç¬¬äºŒä¸ª podã€‚

#### 10.3.3 ä½¿ç”¨ pod

##### é€šè¿‡APIæœåŠ¡å™¨ä¸podé€šä¿¡

```bash
#å¼€å¯ proxy
k proxy

#å‘é€è¯·æ±‚åˆ° kubia-0 pod
curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/

curl -X POST -d "Hey there! This greeting was submitted to kubia-0." localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/

curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
#You've hit kubia-0
#Data stored on this pod: Hey there! This greeting was submitted to kubia-0.
```

![é€šè¿‡kubectlä»£ç†å’ŒapiæœåŠ¡å™¨ä¸podé€šä¿¡](é€šè¿‡kubectlä»£ç†å’ŒapiæœåŠ¡å™¨ä¸podé€šä¿¡.png)

##### æ‰©ç¼©å®¹ StatefulSet

> ç¼©å®¹ä¸€ä¸ªä¼šé¦–å…ˆåˆ é™¤æœ€é«˜ç´¢å¼•çš„ podï¼Œç­‰è¿™ä¸ª pod è¢«å®Œå…¨ç»ˆæ­¢ä¹‹åæ‰ä¼šå¼€å§‹åˆ é™¤æ‹¥æœ‰æ¬¡é«˜ç´¢å¼•çš„ podã€‚

##### é€šè¿‡éheadlessçš„serviceæš´éœ²StatefulSetçš„pod

> é€šè¿‡ API æœåŠ¡å™¨ï¼Œæˆ‘ä»¬å¯ä»¥éšæœºçš„è®¿é—® StatefulSet çš„ podï¼Œåç»­æˆ‘ä»¬ä¼šæ”¹è¿›å®ƒã€‚

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-public
spec:
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
```

```bash
k proxy

curl localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
#You've hit kubia-1
#Data stored on this pod: No data posted yet

curl localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
#You've hit kubia-0
#Data stored on this pod: Hey there! This greeting was submitted to kubia-0.
```

### 10.4 åœ¨ StatefulSet ä¸­å‘ç°ä¼™ä¼´èŠ‚ç‚¹

##### ä»‹ç»SRVè®°å½•

> SRVè®°å½•ç”¨æ¥æŒ‡å‘æä¾›æŒ‡å®šæœåŠ¡çš„æœåŠ¡å™¨çš„ä¸»æœºåå’Œç«¯å£å·ï¼Œkubernetes é€šè¿‡ä¸€ä¸ª headless service åˆ›å»º SRV è®°å½•æ¥æŒ‡å‘ pod çš„ä¸»æœºå

```bash
#
k run --rm -it srvlookup --image=tutum/dnsutils --restart=Never -- dig SRV kubia.default.svc.cluster.local
```

>;; ANSWER SECTION:
>kubia.default.svc.cluster.local. 30 IN	SRV	0 50 8080 kubia-0.kubia.default.svc.cluster.local.
>kubia.default.svc.cluster.local. 30 IN	SRV	0 50 8080 kubia-1.kubia.default.svc.cluster.local.
>
>
>
>;; ADDITIONAL SECTION:
>kubia-1.kubia.default.svc.cluster.local. 30 IN A 10.244.3.30
>kubia-0.kubia.default.svc.cluster.local. 30 IN A 10.244.3.29

#### 10.4.1 é€šè¿‡DNSå®ç°ä¼™ä¼´é—´çš„å½¼æ­¤å‘ç°

```javascript
const http = require('http');
const os = require('os');
const fs = require('fs');
const dns = require('dns');

const dataFile = "/var/data/kubia.txt";
const serviceName = "kubia.default.svc.cluster.local";
const port = 8080;


function fileExists(file) {
  try {
    fs.statSync(file);
    return true;
  } catch (e) {
    return false;
  }
}

// å¯¹äº get è¯·æ±‚è¿”å›
function httpGet(reqOptions, callback) {
  return http.get(reqOptions, function(response) {
    var body = '';
    response.on('data', function(d) { body += d; });
    response.on('end', function() { callback(body); });
  }).on('error', function(e) {
    callback("Error: " + e.message);
  });
}

var handler = function(request, response) {
  // å¦‚æœæ˜¯ POST è¯·æ±‚é‚£ä¹ˆå°†æ•°æ®å†™å…¥æ–‡ä»¶
  if (request.method == 'POST') {
    var file = fs.createWriteStream(dataFile);
    file.on('open', function (fd) {
      request.pipe(file);
      response.writeHead(200);
      response.end("Data stored on pod " + os.hostname() + "\n");
    });
  } else {
    response.writeHead(200);
	// å¦‚æœ url ä»¥ /data ç»“å°¾åˆ™è¿”å›å­˜å‚¨çš„æ•°æ®
    if (request.url == '/data') {
      var data = fileExists(dataFile) ? fs.readFileSync(dataFile, 'utf8') : "No data posted yet";
      response.end(data);
    } else {
      response.write("You've hit " + os.hostname() + "\n");
      response.write("Data stored in the cluster:\n");
	  // ä¸SRVè®°å½•å¯¹åº”çš„æ¯ä¸ªpodé€šä¿¡è·å–å…¶æ•°æ®
      dns.resolveSrv(serviceName, function (err, addresses) {
        if (err) {
          response.end("Could not look up DNS SRV records: " + err);
          return;
        }
        var numResponses = 0;
        if (addresses.length == 0) {
          response.end("No peers discovered.");
        } else {
          addresses.forEach(function (item) {
            var requestOptions = {
              host: item.name,
              port: port,
              path: '/data'
            };
            httpGet(requestOptions, function (returnedData) {
              numResponses++;
              response.write("- " + item.name + ": " + returnedData + "\n");
              if (numResponses == addresses.length) {
                response.end();
              }
            });
          });
        }
      });
    }
  }
};

var www = http.createServer(handler);
www.listen(port);
```

![ç®€å•çš„åˆ†å¸ƒå¼æ•°æ®å­˜å‚¨æœåŠ¡çš„æ“ä½œæµç¨‹](ç®€å•çš„åˆ†å¸ƒå¼æ•°æ®å­˜å‚¨æœåŠ¡çš„æ“ä½œæµç¨‹.png)

#### 10.4.2 æ›´æ–° StatefulSet

#### 10.4.3 å°è¯•é›†ç¾¤æ•°æ®å­˜å‚¨

```bash
#å¤šæ¬¡æ‰§è¡Œå†™å…¥æ•°æ®
curl -X POST -d "The sun is shining" localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
curl -X POST -d "The sun is shining" localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
curl -X POST -d "The sun is shining" localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
#...

#åˆ†å¸ƒå¼æŸ¥è¯¢
curl localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
#You've hit kubia-0
#Data stored in the cluster:
#- kubia-0.kubia.default.svc.cluster.local: The sun is shining
#- kubia-1.kubia.default.svc.cluster.local: The sun is shining
#- kubia-2.kubia.default.svc.cluster.local: No data posted yet
```

### 10.5 äº†è§£ StatefulSet å¦‚ä½•å¤„ç†æ•…éšœèŠ‚ç‚¹

> StatefulSet åœ¨èŠ‚ç‚¹æ•…éšœçš„æ—¶å€™ï¼Œç”±äºèŠ‚ç‚¹æ— æ³•é€šä¿¡ï¼Œæ‰€ä»¥èŠ‚ç‚¹çŠ¶æ€ä¼šå˜æˆ `unknown` ï¼Œåœ¨æŒç»­ä¸€æ®µæ—¶é—´ä¹‹åï¼ŒmasterèŠ‚ç‚¹ä¸Šçš„K8Sæ§åˆ¶å¹³é¢å°±ä¼šè‡ªåŠ¨å°†Podä»æ•…éšœèŠ‚ç‚¹ä¸Šé©±é€ï¼Œä½†æ˜¯å› ä¸ºæ•…éšœèŠ‚ç‚¹ä¸Šçš„Kubeletè¿ä¸ä¸ŠmasterèŠ‚ç‚¹äº†ï¼ŒPodçš„çŠ¶æ€å°±ä¸€ç›´ä¿æŒåœ¨`Terminating`ï¼ŒPodè¿˜ä¼šç»§ç»­è¿è¡Œã€‚
>
> è¿™æ—¶å€™å³ä½¿ä½ ä¸»åŠ¨æ‰§è¡Œ`kubectl delete po`åˆ é™¤Podï¼Œä¹Ÿå¹¶ä¸ä¼šèµ·ä½œç”¨ï¼ŒPodçš„çŠ¶æ€è¿˜æ˜¯`Terminating`ï¼Œå› ä¸ºAPI serveræ”¶ä¸åˆ°KubeletçœŸå®åˆ é™¤Podçš„åé¦ˆã€‚æ‰€ä»¥åªèƒ½æ‰§è¡Œå¼ºåˆ¶åˆ é™¤Podçš„å‘½ä»¤ï¼š

```bash
# --grace-period=0ï¼ŒPodä¼šåœ¨å…¶è¿›ç¨‹å®Œæˆåæ­£å¸¸ç»ˆæ­¢ï¼ŒK8Sé»˜è®¤æ­£å¸¸ç»ˆæ­¢æ—¶é—´æ®µä¸º30sï¼Œåˆ é™¤Podæ—¶å¯ä»¥æ›¿æ¢æ­¤å®½é™æœŸï¼Œå°†--grace-periodæ ‡å¿—è®¾ç½®ä¸ºå¼ºåˆ¶ç»ˆæ­¢Podä¹‹å‰ç­‰å¾…Podç»ˆæ­¢çš„ç§’æ•°
kubectl delete po kubia-0 --force --grace-period=0
# å¦‚æœæ‰§è¡Œäº†ä¸Šé¢çš„å‘½ä»¤åPodè¿˜æ˜¯å¡åœ¨Unknownçš„çŠ¶æ€ï¼Œç”¨ä¸‹é¢çš„å‘½ä»¤å°†Podä»é›†ç¾¤ä¸­ç§»é™¤
kubectl patch pod <pod> -p '{"metadata":{"finalizers":null}}'
```















































